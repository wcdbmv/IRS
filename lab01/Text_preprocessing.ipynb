{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с текстовыми данными обычно начинается с предобработки. Грубо говоря, предобработка - это удаление всего лишнего и приведение к нужному формату. Предобработка может быть долгой и неприятной, но в большинстве случаев все сводится к использованию стандартных инструментов. Эта тетрадка познакомит вас с такими инструментами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Установите все нужные библиотеки. Подробнее про каждую из них ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymystem3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pymystem3) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests->pymystem3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests->pymystem3) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests->pymystem3) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests->pymystem3) (2024.2.2)\n",
      "Collecting pymorphy2\n",
      "  Using cached pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
      "  Using cached DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
      "  Using cached pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting docopt>=0.6 (from pymorphy2)\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Using cached pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "Using cached DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Using cached pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
      "Requirement already satisfied: razdel in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: gensim in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from gensim) (7.0.1)\n",
      "Requirement already satisfied: wrapt in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: nltk in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: rusenttokenize in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (0.0.5)\n",
      "Requirement already satisfied: regex in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (2023.12.25)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymystem3\n",
    "!pip install pymorphy2 # \"pymorphy2[fast]\" doesn't work on macOS arm\n",
    "!pip install razdel\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install rusenttokenize\n",
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скорее всего вы уже знакомы с библиотекой pymorphy2.\n",
    "# pymorphy2[fast] - это оптимизированный pymorphy2, который работает точно также, но сильно быстрее\n",
    "# Если у вас windows, то он вряд ли установится и вам придется пользоваться стандартным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регулярные выражения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из базовых, но в то же время самых полезных инструментов для предобработки текста - регулярные выражения. В вводной части курса им был посвящен целый семинар - https://github.com/mannefedov/compling_nlp_hse_course/blob/master/notebooks/first_module_intro/01_regular_expressions.ipynb  \n",
    "\n",
    "Если вы не ходили подготовительную часть и чувствуете себя неуверенно при работе с регулярками - пройдитесь по семинару. Решить домашку тоже не помешает. В семинаре я буду предполагать, что вы уже знакомы с вещами, которые разобраны в подготовительной части. Но новые вещи я буду объяснять дополнительно!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение на предложения, токенизация, нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За каждым из этих трех терминов стоит большая и сложная подзадача NLP. Однако для каждой есть готовые решения, которые очень хорошо работают. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE. Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\n",
    "\n",
    "Во-первых, NLI можно использовать для контроля качества генеративных моделей. Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод. Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\". С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\n",
    "\n",
    "Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов. Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше. В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично. Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\n",
    "\n",
    "\"\"\"\n",
    "# текст отсюда - https://habr.com/ru/post/582620/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В __nltk__ есть уже готовая функция для разбивки на предложения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/a.kerimov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/a.kerimov/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download as nltk_download\n",
    "nltk_download('punkt')\n",
    "nltk_download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/a.kerimov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".',\n",
       " 'С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).',\n",
       " 'Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов.',\n",
       " 'Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше.',\n",
       " 'В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично.',\n",
       " 'Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text, 'russian')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk также позволяет обучить свой токенизатор предложений под определенный корпус. Это делается не очень просто, но вот тут есть исчерпывающий туториал - https://nlpforhackers.io/splitting-text-into-sentences/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В __gensim__ тоже есть готовая функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtextcleaner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_sentences\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "from gensim.summarization.textcleaner import split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# это ещё и генератор, т.е. сразу подходит для больших корпусов\n",
    "list(split_sentences(text))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У DeepPavlov есть библиотека [**rusenttokenizer**](https://github.com/deepmipt/ru_sentence_tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rusenttokenize import ru_sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Something went wrong while tokenizing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".',\n",
       " 'С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).',\n",
       " 'Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов.',\n",
       " 'Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше.',\n",
       " 'В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично.',\n",
       " 'Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_sent_tokenize(text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В проекте Natasha есть библиотека [**razdel**](https://github.com/natasha/razdel). Она чуть более навороченная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import sentenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = list(sentenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0,\n",
       "           244,\n",
       "           'Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.'),\n",
       " Substring(245,\n",
       "           322,\n",
       "           'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.'),\n",
       " Substring(324,\n",
       "           401,\n",
       "           'Во-первых, NLI можно использовать для контроля качества генеративных моделей.'),\n",
       " Substring(402,\n",
       "           635,\n",
       "           'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.'),\n",
       " Substring(636,\n",
       "           913,\n",
       "           'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# числа тут - это спаны, индексы начала и конца предложения в изначальном тексте\n",
    "sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# у объекта Substring есть атрибуты start, stop и text. С помощью них можно вытащить нужное\n",
    "[sent.text for sent in sents[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если все-таки нужно добавить каких-то специфичных правил разбиения на предложения, можно опять же воспользоваться регулярными выражениями. Однако в этом случае регулярка будет посложнее. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что получится, если в качестве разделителя использовать !?. пробел и заглавную букву.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE',\n",
       " 'роме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\\n\\nВо-первых, NLI можно использовать для контроля качества генеративных моделей',\n",
       " 'сть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод',\n",
       " 'овременные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\"',\n",
       " ' помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\\n\\nВо-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов',\n",
       " 'ля русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше',\n",
       " ' статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично',\n",
       " 'оэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\\n\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'[!?\\.] [А-Я]', text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема в том, что сам разделитель удаляется тоже, а нам нужно удалить только пробел между знаками препинания и заглавной буквой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решается эта проблема с помощью __look ahead__ и __look behind__ (название функционала в регулярных выражениях).  \n",
    "Синтаксис там такой:  \n",
    " **(?<=pattern)** положительное look-behind условие  \n",
    " **(?<!pattern)** отрицательное look-behind условие   \n",
    " **(?=pattern)** положительное look-ahead условие   \n",
    " **(?!pattern)** отрицательное look-ahead условие   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробно про это написано тут: https://www.regular-expressions.info/lookaround.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look behind и look ahead превращают паттерн в условный, то есть проверяется есть ли он (до или после, соответственно), но его захвата не происходит. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обернём наше регулярное выражение и посмотрим, что получается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".',\n",
       " 'С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).',\n",
       " 'Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов.',\n",
       " 'Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше.',\n",
       " 'В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично.',\n",
       " 'Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.  ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'(?<=[\\.?!]) +(?=[А-ЯЁ])', text.replace('\\n', ' '))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы разбили текст на предложения. Теперь предложения нужно разбить на токены. Под токенами обычно понимаются слова, но это могут быть и какие-то более длинные или короткие куски. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой способ токенизации -- стандартный питоновский __str.split__ метод.  \n",
    "По умолчанию он разбивает текст по последовательностям пробелов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '', '2', '3']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1  2 3'.split(' ') # NB! .split() и .split(' ') - не одно и тоже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1  2 3'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рассмотреть,',\n",
       " 'какие',\n",
       " 'языковые',\n",
       " 'явления',\n",
       " 'данная',\n",
       " 'модель',\n",
       " 'понимает',\n",
       " 'хорошо,',\n",
       " 'а',\n",
       " 'на',\n",
       " 'каких',\n",
       " '–',\n",
       " '\"плывёт\";',\n",
       " 'по',\n",
       " 'этому',\n",
       " 'принципу',\n",
       " 'устроены',\n",
       " 'диагностические',\n",
       " 'датасеты',\n",
       " 'SuperGLUE']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()[10:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большая часть слов отделяется, но знаки препинания лепятся к словам.\n",
    "Можно пройтись по всем словам и убрать из них пунктуацию с методом str.strip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#основные знаки преминания хранятся в питоновском модуле string в punctuation\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в этом списке не хватает кавычек-ёлочек, лапок, длинного тире и многоточия\n",
    "string.punctuation += '«»—…“”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рассмотреть',\n",
       " 'какие',\n",
       " 'языковые',\n",
       " 'явления',\n",
       " 'данная',\n",
       " 'модель',\n",
       " 'понимает',\n",
       " 'хорошо',\n",
       " 'а',\n",
       " 'на',\n",
       " 'каких',\n",
       " '–',\n",
       " 'плывёт',\n",
       " 'по',\n",
       " 'этому',\n",
       " 'принципу',\n",
       " 'устроены',\n",
       " 'диагностические',\n",
       " 'датасеты',\n",
       " 'SuperGLUE']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.strip(string.punctuation) for word in text.split()][10:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так не будут удаляться дефисы и точки в сокращениях, не разделенных пробелом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'как-нибудь'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'как-нибудь'.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'т.е'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'т.е.'.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой способ токенизации может работать быстрее других из-за того, что используются только дефолтные инструменты питона. Если важно качество, то лучше пользоваться готовыми токенизаторами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, готовые токенизаторы есть в nltk. Они не удаляют пунктуацию, а выделяют её отдельным токеном.\n",
    "\n",
    "**wordpunct_tokenizer** разбирает по регулярке - *'\\w+|[^\\w\\s]+'* (попробуйте понять как она работает просто глядя на паттерн)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача',\n",
       " 'NLI',\n",
       " 'важна',\n",
       " 'для',\n",
       " 'компьютерных',\n",
       " 'лингвистов',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволяет']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word_tokenize** также построен на регулярках, но они там более сложные (учитывается последовательность некоторых \n",
    "символов, символы начала, конца слова и т.д). \n",
    "\n",
    "Специально подобранного под русский язык токенизатора там нет, \n",
    "но и с английским всё работает достаточно хорошо --\n",
    "сокращения типа т.к собираются в один токен, дефисные слова тоже не разделяются, многоточия тут тоже не отделяются, но это можно поправить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лажают',\n",
       " ',',\n",
       " 'упуская',\n",
       " 'какую-то',\n",
       " 'важную',\n",
       " 'информацию',\n",
       " 'из',\n",
       " 'Х',\n",
       " ',',\n",
       " 'или',\n",
       " ',',\n",
       " 'наоборот',\n",
       " ',',\n",
       " 'дописывая',\n",
       " 'в',\n",
       " 'текст',\n",
       " 'Y',\n",
       " 'что-то',\n",
       " 'нафантазированное',\n",
       " '``']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)[130:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В генсиме тоже есть функция для токенизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['russiansuperglue',\n",
       " 'кроме',\n",
       " 'этого',\n",
       " 'модели',\n",
       " 'nli',\n",
       " 'обладают',\n",
       " 'прикладной',\n",
       " 'ценностью',\n",
       " 'по',\n",
       " 'нескольким',\n",
       " 'причинам',\n",
       " 'во',\n",
       " 'первых',\n",
       " 'nli',\n",
       " 'можно',\n",
       " 'использовать',\n",
       " 'для',\n",
       " 'контроля',\n",
       " 'качества',\n",
       " 'генеративных']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# опять же, это генератор\n",
    "list(tokenize(text, lowercase=True))[30:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И в razdel тоже есть токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize as razdel_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 6, 'Задача'),\n",
       " Substring(7, 10, 'NLI'),\n",
       " Substring(11, 16, 'важна'),\n",
       " Substring(17, 20, 'для'),\n",
       " Substring(21, 33, 'компьютерных'),\n",
       " Substring(34, 44, 'лингвистов'),\n",
       " Substring(44, 45, ','),\n",
       " Substring(46, 49, 'ибо'),\n",
       " Substring(50, 53, 'она'),\n",
       " Substring(54, 63, 'позволяет')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(razdel_tokenize(text))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача',\n",
       " 'NLI',\n",
       " 'важна',\n",
       " 'для',\n",
       " 'компьютерных',\n",
       " 'лингвистов',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволяет']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in list(razdel_tokenize(text))[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работать с регистром тяжело и поэтому можно привести все к нижнему регистру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача',\n",
       " 'nli',\n",
       " 'важна',\n",
       " 'для',\n",
       " 'компьютерных',\n",
       " 'лингвистов',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволяет']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text.lower() for token in list(razdel_tokenize(text))[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В последнее время нормализация (т.е. приведение токенов к стандартному виду) используется все реже. Это связано с использованием subword или byte токенизации в топовых моделях (подробнее об этом мы поговорим когда дойдем до нейронных сетей). Однако у них есть свои недостатки и забывать про нормализацию пока не стоит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Два основных вида нормализации - лемматизация и стемминг. Стемминг уже нигде не используется, но его полезно разобрать, чтобы понимать почему нужно использовать лемматизацию (еще стемминг по непонятной причине часто упоминается как важный скилл в вакансиях)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стемминг - это урезание слова до его \"основы\" (стема), т.е. такой части, которая является общей для всех словоформ в парадигме слова *(Значения слов \"слово\", \"слоформа\", \"парадигма\" приблизительно соответствует тому, которое использует Зализняк вот тут - http://inslav.ru/images/stories/pdf/2002_Zalizniak_RIS_i_statji.pdf (стр. 21-22)) Но это на самом деле не важно)*. \n",
    "\n",
    "По крайней мере так в теории. На практике стемминг сводится к отбрасыванию частотных окончаний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый известный стеммер - стеммер Портера (или snowball стеммер). \n",
    "Подробнее про стеммер Портера можно почитать вот тут - <https://medium.com/@eigenein/стеммер-портера-для-русского-языка-d41c38b2d340>  \n",
    "А совсем подробнее вот тут - <http://snowball.tartarus.org/algorithms/russian/stemmer.html>  \n",
    "Почему он так называется? Так назывался язык программирования, который Портер написал для стеммеров. Язык так называется в созвучие языку SNOBOL. Вот комментарий самого Портера:\n",
    "\n",
    "`Since it effectively provides a ‘suffix STRIPPER GRAMmar’, I had toyed with the idea of calling it ‘strippergram’, but good sense has prevailed, and so it is ‘Snowball’ named as a tribute to SNOBOL, the excellent string handling language of Messrs Farber, Griswold, Poage and Polonsky from the 1960s.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовые стеммеры для разных языков есть в nltk. Работают они вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Задача', 'задач'),\n",
       " ('NLI', 'NLI'),\n",
       " ('важна', 'важн'),\n",
       " ('для', 'для'),\n",
       " ('компьютерных', 'компьютерн'),\n",
       " ('лингвистов', 'лингвист'),\n",
       " (',', ','),\n",
       " ('ибо', 'иб'),\n",
       " ('она', 'он'),\n",
       " ('позволяет', 'позволя'),\n",
       " ('детально', 'детальн'),\n",
       " ('рассмотреть', 'рассмотрет'),\n",
       " (',', ','),\n",
       " ('какие', 'как'),\n",
       " ('языковые', 'языков'),\n",
       " ('явления', 'явлен'),\n",
       " ('данная', 'дан'),\n",
       " ('модель', 'модел'),\n",
       " ('понимает', 'понима'),\n",
       " ('хорошо', 'хорош'),\n",
       " (',', ','),\n",
       " ('а', 'а'),\n",
       " ('на', 'на'),\n",
       " ('каких', 'как'),\n",
       " ('–', '–'),\n",
       " ('``', '``'),\n",
       " ('плывёт', 'плывет'),\n",
       " (\"''\", \"''\"),\n",
       " (';', ';'),\n",
       " ('по', 'по')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(word, stemmer.stem(word)) for word in word_tokenize(text)][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостатки стемминга достаточно очевидные:  \n",
    "1) с супплетивными формами или редкими окончаниями слова стемминг работать не умеет  \n",
    "2) к одной основе могут приводится разные слова  \n",
    "3) к разным основам могут сводиться формы одного слова  \n",
    "4) приставки не отбрасываются"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация - это замена словоформы слова в парадигме на какую-то заранее выбранную стадартную форму (лемму). \n",
    "\n",
    "\n",
    "\n",
    "Например, для разных форм глагола леммой обычно является неопределенная форма (инфинитив), а для существительного форма мужского рода единственного числа. Это позволяет избавиться от недостатков стемминга (будет, был - одна лемма), (пролить, пролом - разные). Однако лемматизация значительно сложнее. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К счастью есть готовые хорошие лемматизаторы. Для русского основых варианта два: Mystem и Pymorphy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import os, json\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Майстем работает немного лучше и сам токенизирует,\n",
    "поэтому можно в него засовывать сырой текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача', ' ', 'NLI', ' ', 'важный', ' ', 'для', ' ', 'компьютерный', ' ']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mystem.lemmatize функция лемматизации в майстеме\n",
    "# сам объект mystem нужно заранее инициализировать\n",
    "mystem.lemmatize(text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужна грамматическая информация или надо сохранить ненормализованный текст,\n",
    "# есть функция mystem.analyze\n",
    "words_analized = mystem.analyze(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'задача', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}],\n",
       "  'text': 'Задача'},\n",
       " {'text': ' '},\n",
       " {'analysis': [], 'text': 'NLI'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'важный', 'wt': 1, 'gr': 'A=ед,кр,жен'}],\n",
       "  'text': 'важна'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'для', 'wt': 1, 'gr': 'PR='}], 'text': 'для'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'компьютерный',\n",
       "    'wt': 1,\n",
       "    'gr': 'A=(пр,мн,полн|вин,мн,полн,од|род,мн,полн)'}],\n",
       "  'text': 'компьютерных'},\n",
       " {'text': ' '}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# возвращает она список словарей\n",
    "# каждый словарь имеет либо одно поле 'text' (когда попался пробел) или text и analysis\n",
    "# в analysis снова список словарей с вариантами разбора (первый самый вероятный)\n",
    "# поля в analysis - 'gr' - грамматическая информация, 'lex' - лемма\n",
    "# analysis - может быть пустым списком\n",
    "words_analized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово -  Задача\n",
      "Разбор слова -  {'lex': 'задача', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}\n",
      "Лемма слова -  задача\n",
      "Грамматическая информация слова -  S,жен,неод=им,ед\n"
     ]
    }
   ],
   "source": [
    "print('Слово - ', words_analized[0]['text'])\n",
    "print('Разбор слова - ', words_analized[0]['analysis'][0])\n",
    "print('Лемма слова - ', words_analized[0]['analysis'][0]['lex'])\n",
    "print('Грамматическая информация слова - ', words_analized[0]['analysis'][0]['gr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача',\n",
       " 'важный',\n",
       " 'для',\n",
       " 'компьютерный',\n",
       " 'лингвист',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволять',\n",
       " 'детально',\n",
       " 'рассматривать']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#леммы можно достать в одну строчку\n",
    "[parse['analysis'][0]['lex'] for parse in words_analized if parse.get('analysis')][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mystem умеет разбивать текст на предложения, но через питоновский интерфейс это сделать не получится. Нужно скачать mystem отсюда - https://yandex.ru/dev/mystem/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого сохранить текст в файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('text.txt', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из командной строки или из питона запустить майстем на нашем файле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# про параметры почитайте в !mystem -h\n",
    "!/Users/a.kerimov/.local/bin/mystem -isc --format json text.txt text_parsed.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целевом файле теперь лежит разобранный текст в jsonlines (json на каждой строчке)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "t = [json.loads(line) for line in open('text_parsed.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'задача', 'gr': 'S,жен,неод=им,ед'}], 'text': 'Задача'},\n",
       " {'text': ' '},\n",
       " {'analysis': [], 'text': 'NLI'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'важный', 'gr': 'A=ед,кр,жен'}], 'text': 'важна'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'для', 'gr': 'PR='}], 'text': 'для'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'компьютерный', 'gr': 'A=пр,мн,полн'},\n",
       "   {'lex': 'компьютерный', 'gr': 'A=вин,мн,полн,од'},\n",
       "   {'lex': 'компьютерный', 'gr': 'A=род,мн,полн'}],\n",
       "  'text': 'компьютерных'},\n",
       " {'text': ' '}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый объект в этом списке - параграф. Каждый параграф на предложения можно разбив по тегу '//s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё так вызывать майстем может понадобиться, если важна скорость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостатки Mystem: это продукт Яндекса с некоторыми ограничениями на использование, больше он не развивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важным достоинством Mystem является то, что он работает не с отдельными словами, а с целым предложением. При определении нужной леммы учитывается контекст, что позволяет во многих случаях разрешать омонимию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pymorphy - открытый и развивается (но не очень активно, т.к. это все сложно)\n",
    "\n",
    "Ссылка на репозиторий: https://github.com/kmike/pymorphy2\n",
    "\n",
    "Попробуйте сразу установить быструю версию (pip install pymorphy2[fast])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У него нет втстроенной токенизации и он расценивает всё как слово. Когда есть несколько вариантов, он выдает их с вероятностостями, которые расчитатны на корпусе со снятой неоднозначностью. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# основная функция - pymorphy.parse\n",
    "words_analized = [morph.parse(token) for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='печь', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='печь', score=0.571428, methods_stack=((DictionaryAnalyzer(), 'печь', 2223, 0),)),\n",
       " Parse(word='печь', tag=OpencorporaTag('INFN,impf,tran'), normal_form='печь', score=0.285714, methods_stack=((DictionaryAnalyzer(), 'печь', 2456, 0),)),\n",
       " Parse(word='печь', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form='печь', score=0.142857, methods_stack=((DictionaryAnalyzer(), 'печь', 2223, 3),))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse(\"печь\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первое слово -  задача\n",
      "Разбор первого слова -  Parse(word='задача', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='задача', score=1.0, methods_stack=((DictionaryAnalyzer(), 'задача', 94, 0),))\n",
      "Лемма первого слова -  задача\n",
      "Грамматическая информация первого слова -  NOUN,inan,femn sing,nomn\n",
      "Часть речи первого слова -  NOUN\n",
      "Род первого слова -  femn\n",
      "Число первого слова -  sing\n",
      "Падеж первого слова -  nomn\n"
     ]
    }
   ],
   "source": [
    "# Она похожа на analyze в майстеме только возрващает список объектов Parse\n",
    "# Первый в списке - самый вероятный разбор (у каждого есть score)\n",
    "# Информация достается через атрибут (Parse.word - например)\n",
    "# Грамматическая информация хранится в объекте OpencorporaTag и из него удобно доставать\n",
    "# части речи или другие категории\n",
    "print('Первое слово - ', words_analized[0][0].word)\n",
    "print('Разбор первого слова - ', words_analized[0][0])\n",
    "print('Лемма первого слова - ', words_analized[0][0].normal_form)\n",
    "print('Грамматическая информация первого слова - ', words_analized[0][0].tag)\n",
    "print('Часть речи первого слова - ', words_analized[0][0].tag.POS)\n",
    "print('Род первого слова - ', words_analized[0][0].tag.gender)\n",
    "print('Число первого слова - ', words_analized[0][0].tag.number)\n",
    "print('Падеж первого слова - ', words_analized[0][0].tag.case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная очистка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно убрать стоп-слова (предлоги, союзы, местоимения, частотные слова). Сам термин стоп-слово происходит из информационного поиска, первый раз его упомянул [Питер Лун](https://en.wikipedia.org/wiki/Hans_Peter_Luhn) в 1959.  \n",
    "Удаление таких слов позволяло сократить размер индекса и не сильно испортить выдачу или даже улучшить её, поднимая релевантность документам со значимыми словами. Со временем от такой практики, в основном, отказались - память стала дешевой (и повились всякие алгоритмы для сокращения потребления памяти), а для учёта значимости придумали TFIDF (про него на следующем занятии).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во многих поисковых движках стоп-слова всё ещё используются. Часто их используют и в практических задачах (классификации, тематическом моделировании). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "# стоп-слова есть в nltk\n",
    "stops = stopwords.words('russian')\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список не идеальный и его можно расширять под свои задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача',\n",
       " 'nli',\n",
       " 'важный',\n",
       " 'компьютерный',\n",
       " 'лингвист',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'позволять',\n",
       " 'детально',\n",
       " 'рассмотреть']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_normalized = [morph.parse(token)[0].normal_form for token in word_tokenize(text)]\n",
    "[word for word in words_normalized if word not in stops][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка для других языков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка сильно зависит от языка. Полностью универсальнных токенизаторов, лемматизаторов не бывает, а рассказать о предобработке под все существующие языки не реально. Поэтому посмотрим дополнительно только на предобработку на английском языке. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk и gensim по умолчанию адаптированы под английский язык (а регулярки вообще не привязаны к языку), поэтому разбирать их еще раз не будем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека, про которую стоит отдельно рассказать - [**SpaCy**](https://spacy.io/). Это многоцелевая многоязычная библиотека. Если вам понадобится серьезно работать с английским, то лучшим вариантом будет использовать SpaCy. Другие языки там тоже поддерживаются (см. документацию), но не настолько хорошо как английский язык. \n",
    "\n",
    "В SpaCy много всего и мы будем возвращаться к ней по ходу курса. Пока посмотрим на интрументы базовой предобработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp310-cp310-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.6.3-py3-none-any.whl.metadata (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m966.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.16.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Downloading spacy-3.7.4-cp310-cp310-macosx_11_0_arm64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp310-cp310-macosx_11_0_arm64.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.10-cp310-cp310-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-macosx_11_0_arm64.whl (127 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.2/395.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.3-cp310-cp310-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp310-cp310-macosx_11_0_arm64.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.3-cp310-cp310-macosx_11_0_arm64.whl (789 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.6/789.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp310-cp310-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, langcodes, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 7.0.1\n",
      "    Uninstalling smart-open-7.0.1:\n",
      "      Successfully uninstalled smart-open-7.0.1\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.6.3 pydantic-core-2.16.3 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from de-core-news-sm==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Collecting ru-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.7.0/ru_core_news_sm-3.7.0-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from ru-core-news-sm==3.7.0) (3.7.4)\n",
      "Collecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.7.0)\n",
      "  Downloading pymorphy3-2.0.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0) (0.7.2)\n",
      "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n",
      "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.1.5)\n",
      "Downloading pymorphy3-2.0.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m811.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, pymorphy3, ru-core-news-sm\n",
      "Successfully installed pymorphy3-2.0.1 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем пайплайн для английского языка\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = (\"One of the most salient features of our culture is that it won't so much bullshit.” \"\n",
    "        \"These are the opening words of the short book On Bullshit, written by the philosopher Harry Frankfurt. \"\n",
    "        \"Fifteen years after the publication of this surprise bestseller, \"\n",
    "        \"the rapid progress of research on artificial intelligence is forcing us to reconsider our conception \"\n",
    "        \"of bullshit as a hallmark of human speech, with troubling implications. What do philosophical \"\n",
    "        \"reflections on bullshit have to do with algorithms? As it turns out, quite a lot.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат предобаботки очень похож на майстем - тут есть разбиение на предложения, на токены, лемматизация, определение части речи. Тэги тут используются другие, но при желании можно сделать более менее адекватный маппинг (возможно кто-то уже это сделал и нужно только погуглить)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One one NUM\n",
      "of of ADP\n",
      "the the DET\n",
      "most most ADV\n",
      "salient salient ADJ\n",
      "features feature NOUN\n",
      "of of ADP\n",
      "our our PRON\n",
      "culture culture NOUN\n",
      "is be AUX\n",
      "that that SCONJ\n",
      "it it PRON\n",
      "wo will AUX\n",
      "n't not PART\n",
      "so so ADV\n",
      "much much ADJ\n",
      "bullshit bullshit NOUN\n",
      ". . PUNCT\n",
      "” \" PUNCT\n",
      "\n",
      "These these PRON\n",
      "are be AUX\n",
      "the the DET\n",
      "opening opening NOUN\n",
      "words word NOUN\n",
      "of of ADP\n",
      "the the DET\n",
      "short short ADJ\n",
      "book book NOUN\n",
      "On on ADP\n",
      "Bullshit Bullshit PROPN\n",
      ", , PUNCT\n",
      "written write VERB\n",
      "by by ADP\n",
      "the the DET\n",
      "philosopher philosopher NOUN\n",
      "Harry Harry PROPN\n",
      "Frankfurt Frankfurt PROPN\n",
      ". . PUNCT\n",
      "\n",
      "Fifteen fifteen NUM\n",
      "years year NOUN\n",
      "after after ADP\n",
      "the the DET\n",
      "publication publication NOUN\n",
      "of of ADP\n",
      "this this DET\n",
      "surprise surprise NOUN\n",
      "bestseller bestseller NOUN\n",
      ", , PUNCT\n",
      "the the DET\n",
      "rapid rapid ADJ\n",
      "progress progress NOUN\n",
      "of of ADP\n",
      "research research NOUN\n",
      "on on ADP\n",
      "artificial artificial ADJ\n",
      "intelligence intelligence NOUN\n",
      "is be AUX\n",
      "forcing force VERB\n",
      "us we PRON\n",
      "to to PART\n",
      "reconsider reconsider VERB\n",
      "our our PRON\n",
      "conception conception NOUN\n",
      "of of ADP\n",
      "bullshit bullshit NOUN\n",
      "as as ADP\n",
      "a a DET\n",
      "hallmark hallmark NOUN\n",
      "of of ADP\n",
      "human human ADJ\n",
      "speech speech NOUN\n",
      ", , PUNCT\n",
      "with with ADP\n",
      "troubling troubling ADJ\n",
      "implications implication NOUN\n",
      ". . PUNCT\n",
      "\n",
      "What what PRON\n",
      "do do AUX\n",
      "philosophical philosophical ADJ\n",
      "reflections reflection NOUN\n",
      "on on ADP\n",
      "bullshit bullshit NOUN\n",
      "have have VERB\n",
      "to to PART\n",
      "do do VERB\n",
      "with with ADP\n",
      "algorithms algorithm NOUN\n",
      "? ? PUNCT\n",
      "\n",
      "As as SCONJ\n",
      "it it PRON\n",
      "turns turn VERB\n",
      "out out ADP\n",
      ", , PUNCT\n",
      "quite quite DET\n",
      "a a DET\n",
      "lot lot NOUN\n",
      ". . PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задачах вроде извлечения ключевых слов может пригодится вытащить из текста только noun phrases (по-русски это вроде называется именные группы существительного)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['the most salient features', 'our culture', 'it', 'These', 'the opening words', 'the short book', 'Bullshit', 'the philosopher', 'Harry Frankfurt', 'the publication', 'this surprise bestseller', 'the rapid progress', 'research', 'artificial intelligence', 'us', 'our conception', 'bullshit', 'a hallmark', 'human speech', 'troubling implications', 'What', 'philosophical reflections', 'bullshit', 'algorithms', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем пайплайн для немецкого языка\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = (\"Vor den Stadien habe ich bis jetzt zum Glück noch keine wüsten Szenen gesehen.\"\n",
    "        \"Vorstandschef Timotheus Höttges habe sich ausgesprochen optimistisch gezeigt, schrieb Analyst Robert Grindle in einer Studie vom Montag. \"\n",
    "        \"Während der dortigen Räterepublik war er nach dem Krieg in Künstlergruppen und Ausschüssen aktiv.\"\n",
    "        \"Welches Ergebnis die Diskussion auf EU-Ebene auch letztlich bringt, wichtig ist, dass die Preisentwicklung für die\"\n",
    "        \"Menschen verträglicher gestaltet wird“, so Gusenbauer.\"\n",
    "        \"Weitere Informationen unter www.schnippenburg.de sowie www.eisenzeithaus.de. Es gibt neue Nachrichten auf noz.de!\" \n",
    "        \"Jetzt die Startseite neu laden.\"\n",
    "        \"Der Initiative 'Zivilcourage', die sich jahrelang für das Denkmal in Form eines offenen \" \n",
    "        \"Der islamistischen Szene Thüringens wurden nach Angaben des Thüringer Innenministeriums \"\n",
    "        \"zuletzt etwa 125 Personen zugerechnet, der salafistischen Szene etwa 75 Personen.\"\n",
    "        \"Allerdings bestand er die EMV-Prüfung nicht, weil er Radios und DVB-T-Empfänger stört.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vor vor ADP\n",
      "den der DET\n",
      "Stadien Stadien NOUN\n",
      "habe haben AUX\n",
      "ich ich PRON\n",
      "bis bis ADP\n",
      "jetzt jetzt ADV\n",
      "zum zu ADP\n",
      "Glück Glück NOUN\n",
      "noch noch ADV\n",
      "keine kein DET\n",
      "wüsten wüst ADJ\n",
      "Szenen Szene NOUN\n",
      "gesehen sehen VERB\n",
      ". -- PUNCT\n",
      "\n",
      "Vorstandschef Vorstandschef NOUN\n",
      "Timotheus Timotheus PROPN\n",
      "Höttges Höttges PROPN\n",
      "habe haben AUX\n",
      "sich sich PRON\n",
      "ausgesprochen ausgesprochen ADV\n",
      "optimistisch optimistisch ADV\n",
      "gezeigt zeigen VERB\n",
      ", -- PUNCT\n",
      "schrieb schreiben VERB\n",
      "Analyst Analyst NOUN\n",
      "Robert Robert PROPN\n",
      "Grindle Grindle PROPN\n",
      "in in ADP\n",
      "einer ein DET\n",
      "Studie Studie NOUN\n",
      "vom von ADP\n",
      "Montag Montag NOUN\n",
      ". -- PUNCT\n",
      "\n",
      "Während während ADP\n",
      "der der DET\n",
      "dortigen dortig ADJ\n",
      "Räterepublik Räterepublik NOUN\n",
      "war sein AUX\n",
      "er er PRON\n",
      "nach nach ADP\n",
      "dem der DET\n",
      "Krieg Krieg NOUN\n",
      "in in ADP\n",
      "Künstlergruppen Künstlergruppe NOUN\n",
      "und und CCONJ\n",
      "Ausschüssen Ausschuß NOUN\n",
      "aktiv aktiv ADV\n",
      ". -- PUNCT\n",
      "\n",
      "Welches welcher DET\n",
      "Ergebnis Ergebnis NOUN\n",
      "die der DET\n",
      "Diskussion Diskussion NOUN\n",
      "auf auf ADP\n",
      "EU-Ebene EU-Eben NOUN\n",
      "auch auch ADV\n",
      "letztlich letztlich ADV\n",
      "bringt bringen VERB\n",
      ", -- PUNCT\n",
      "wichtig wichtig ADV\n",
      "ist sein AUX\n",
      ", -- PUNCT\n",
      "dass dass SCONJ\n",
      "die der DET\n",
      "Preisentwicklung Preisentwicklung NOUN\n",
      "für für ADP\n",
      "dieMenschen dieMenschen NOUN\n",
      "verträglicher verträglich ADV\n",
      "gestaltet gestalten VERB\n",
      "wird werden AUX\n",
      "“ -- PUNCT\n",
      ", -- PUNCT\n",
      "so so ADV\n",
      "Gusenbauer Gusenbauer PROPN\n",
      ". -- PUNCT\n",
      "\n",
      "Weitere weit ADJ\n",
      "Informationen Information NOUN\n",
      "unter unter ADP\n",
      "www.schnippenburg.de www.schnippenburg.de PROPN\n",
      "sowie sowie CCONJ\n",
      "www.eisenzeithaus.de www.eisenzeithaus.de NOUN\n",
      ". -- PUNCT\n",
      "\n",
      "Es es PRON\n",
      "gibt geben VERB\n",
      "neue neu ADJ\n",
      "Nachrichten Nachricht NOUN\n",
      "auf auf ADP\n",
      "noz.de noz.de PROPN\n",
      "! -- PUNCT\n",
      "\n",
      "Jetzt jetzt ADV\n",
      "die der DET\n",
      "Startseite Startseit NOUN\n",
      "neu neu ADV\n",
      "laden laden VERB\n",
      ". -- PUNCT\n",
      "\n",
      "Der der DET\n",
      "Initiative Initiative NOUN\n",
      "' ' PUNCT\n",
      "Zivilcourage Zivilcourage NOUN\n",
      "' -- PUNCT\n",
      ", -- PUNCT\n",
      "die der PRON\n",
      "sich sich PRON\n",
      "jahrelang jahrelang ADV\n",
      "für für ADP\n",
      "das der DET\n",
      "Denkmal Denkmal NOUN\n",
      "in in ADP\n",
      "Form Form NOUN\n",
      "eines ein DET\n",
      "offenen offen ADJ\n",
      "Der der DET\n",
      "islamistischen islamistisch ADJ\n",
      "Szene Szene NOUN\n",
      "Thüringens Thüringen PROPN\n",
      "wurden werden AUX\n",
      "nach nach ADP\n",
      "Angaben Angabe NOUN\n",
      "des der DET\n",
      "Thüringer Thüringer ADJ\n",
      "Innenministeriums Innenministerium NOUN\n",
      "zuletzt zuletzt ADV\n",
      "etwa etwa ADV\n",
      "125 125 NUM\n",
      "Personen Person NOUN\n",
      "zugerechnet zurechnen VERB\n",
      ", -- PUNCT\n",
      "der der DET\n",
      "salafistischen salafistisch ADJ\n",
      "Szene Szene NOUN\n",
      "etwa etwa ADV\n",
      "75 75 NUM\n",
      "Personen Person NOUN\n",
      ". -- PUNCT\n",
      "\n",
      "Allerdings allerdings ADV\n",
      "bestand bestehen VERB\n",
      "er er PRON\n",
      "die der DET\n",
      "EMV-Prüfung EMV-Prüfung NOUN\n",
      "nicht nicht PART\n",
      ", -- PUNCT\n",
      "weil weil SCONJ\n",
      "er er PRON\n",
      "Radios Radios NOUN\n",
      "und und CCONJ\n",
      "DVB-T-Empfänger DVB-T-Empfäng NUM\n",
      "stört stören VERB\n",
      ". -- PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['den Stadien', 'ich', 'Glück', 'noch keine wüsten Szenen', 'Vorstandschef Timotheus Höttges', 'sich', 'Analyst Robert Grindle', 'einer Studie', 'Montag', 'der dortigen Räterepublik', 'er', 'dem Krieg', 'Künstlergruppen', 'Ausschüssen', 'Welches Ergebnis', 'die Diskussion', 'EU-Ebene', 'die Preisentwicklung', 'dieMenschen', 'Weitere Informationen', 'www.schnippenburg.de', 'www.eisenzeithaus.de', 'neue Nachrichten', 'noz.de', 'die Startseite', \"Der Initiative 'Zivilcourage\", 'die', 'sich', 'das Denkmal', 'Form', 'Der islamistischen Szene', 'Thüringens', 'Angaben', 'des Thüringer Innenministeriums', 'etwa 125 Personen', 'der salafistischen Szene', 'etwa 75 Personen', 'er', 'die EMV-Prüfung', 'er', 'Radios']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поддержка русского языка в spacy тоже не так давно добавилась, но она не полная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем любой текст\n",
    "text = \"ДАННОЕ СООБЩЕНИЕ (МАТЕРИАЛ) СОЗДАНО И (ИЛИ) РАСПРОСТРАНЕНО \"\\\n",
    "       \"ИНОСТРАННЫМ СРЕДСТВОМ МАССОВОЙ ИНФОРМАЦИИ, ВЫПОЛНЯЮЩИМ \"\\\n",
    "       \"ФУНКЦИИ ИНОСТРАННОГО АГЕНТА, И (ИЛИ) РОССИЙСКИМ ЮРИДИЧЕСКИМ ЛИЦОМ, \"\\\n",
    "       \"ВЫПОЛНЯЮЩИМ ФУНКЦИИ ИНОСТРАННОГО АГЕНТА\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ДАННОЕ данное NOUN\n",
      "СООБЩЕНИЕ сообщение PROPN\n",
      "( ( PUNCT\n",
      "МАТЕРИАЛ материал PROPN\n",
      ") ) PUNCT\n",
      "СОЗДАНО создано PROPN\n",
      "И и PROPN\n",
      "( ( PUNCT\n",
      "ИЛИ или PROPN\n",
      ") ) PUNCT\n",
      "РАСПРОСТРАНЕНО распространено PROPN\n",
      "ИНОСТРАННЫМ иностранным PROPN\n",
      "СРЕДСТВОМ средством PROPN\n",
      "МАССОВОЙ массовой PROPN\n",
      "ИНФОРМАЦИИ информации PROPN\n",
      ", , PUNCT\n",
      "ВЫПОЛНЯЮЩИМ выполняющим PROPN\n",
      "ФУНКЦИИ функция PROPN\n",
      "ИНОСТРАННОГО иностранного PROPN\n",
      "АГЕНТА агента PROPN\n",
      ", , PUNCT\n",
      "И и CCONJ\n",
      "( ( PUNCT\n",
      "ИЛИ или PROPN\n",
      ") ) PUNCT\n",
      "РОССИЙСКИМ российским PROPN\n",
      "ЮРИДИЧЕСКИМ юридическим PROPN\n",
      "ЛИЦОМ лицом PROPN\n",
      ", , PUNCT\n",
      "ВЫПОЛНЯЮЩИМ выполняющим PROPN\n",
      "ФУНКЦИИ функция PROPN\n",
      "ИНОСТРАННОГО иностранного PROPN\n",
      "АГЕНТА агент PROPN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "[E894] The 'noun_chunks' syntax iterator is not implemented for language 'ru'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoun phrases:\u001b[39m\u001b[38;5;124m\"\u001b[39m, [chunk\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mnoun_chunks])\n",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoun phrases:\u001b[39m\u001b[38;5;124m\"\u001b[39m, [chunk\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mnoun_chunks])\n",
      "File \u001b[0;32m~/bmstu/IRS/env-3.10/lib/python3.10/site-packages/spacy/tokens/doc.pyx:900\u001b[0m, in \u001b[0;36mnoun_chunks\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: [E894] The 'noun_chunks' syntax iterator is not implemented for language 'ru'."
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ошибки токенизации\n",
    "\n",
    "Найдите 1 любой способ сломать токенизацию на предложения функцией sentenize из библиотеки razdel.\n",
    "Придумайте (или найдите на каком-то корпусе) такое предложение (или несколько предложений),\n",
    "которое будет некорректно разобрано sentenize,\n",
    "но при этом будет грамматически верным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import sentenize as razdel_sentenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0,\n",
       "           203,\n",
       "           'Сентенизатор ищет заглавную букву после завершающего знака препинания. однако с точки зрения русского языка первое слово в предложении, начинающееся с маленькой буквы, не является грамматической ошибкой.')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'Сентенизатор ищет заглавную букву после завершающего знака препинания. однако с точки зрения русского языка первое слово в предложении, начинающееся с маленькой буквы, не является грамматической ошибкой.'\n",
    "list(razdel_sentenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0,\n",
       "           118,\n",
       "           'Также, сентенизатор не распознаёт схожие по начертанию символы, не так ли？Это тоже не является грамматической ошибкой.')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = 'Также, сентенизатор не распознаёт схожие по начертанию символы, не так ли？Это тоже не является грамматической ошибкой.'\n",
    "list(razdel_sentenize(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Токенизация Mystem vs razdel.tokenize\n",
    "\n",
    "Токенизируйте текст (не менее 10 предложений, можно взять любую статью Вики) с помощью razdel и с помощью Mystem. Найдите различия в токенизациях. Что, по вашему мнению, работает лучше на этом тексте?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 7, 'Призрак'),\n",
       " Substring(8, 14, 'бродит'),\n",
       " Substring(15, 17, 'по'),\n",
       " Substring(18, 24, 'Европе'),\n",
       " Substring(25, 26, '-'),\n",
       " Substring(27, 34, 'призрак'),\n",
       " Substring(35, 45, 'коммунизма'),\n",
       " Substring(45, 46, '.'),\n",
       " Substring(47, 50, 'Все'),\n",
       " Substring(51, 55, 'силы'),\n",
       " Substring(56, 62, 'старой'),\n",
       " Substring(63, 69, 'Европы'),\n",
       " Substring(70, 82, 'объединились'),\n",
       " Substring(83, 86, 'для'),\n",
       " Substring(87, 96, 'священной'),\n",
       " Substring(97, 103, 'травли'),\n",
       " Substring(104, 109, 'этого'),\n",
       " Substring(110, 118, 'призрака'),\n",
       " Substring(118, 119, ':'),\n",
       " Substring(120, 124, 'папа'),\n",
       " Substring(125, 126, 'и'),\n",
       " Substring(127, 131, 'царь'),\n",
       " Substring(131, 132, ','),\n",
       " Substring(133, 142, 'Меттерних'),\n",
       " Substring(143, 144, 'и'),\n",
       " Substring(145, 149, 'Гизо'),\n",
       " Substring(149, 150, ','),\n",
       " Substring(151, 162, 'французские'),\n",
       " Substring(163, 171, 'радикалы'),\n",
       " Substring(172, 173, 'и'),\n",
       " Substring(174, 182, 'немецкие'),\n",
       " Substring(183, 194, 'полицейские'),\n",
       " Substring(194, 195, '.'),\n",
       " Substring(196, 199, 'Где'),\n",
       " Substring(200, 202, 'та'),\n",
       " Substring(203, 216, 'оппозиционная'),\n",
       " Substring(217, 223, 'партия'),\n",
       " Substring(223, 224, ','),\n",
       " Substring(225, 232, 'которую'),\n",
       " Substring(233, 235, 'ее'),\n",
       " Substring(236, 246, 'противники'),\n",
       " Substring(246, 247, ','),\n",
       " Substring(248, 255, 'стоящие'),\n",
       " Substring(256, 257, 'у'),\n",
       " Substring(258, 264, 'власти'),\n",
       " Substring(264, 265, ','),\n",
       " Substring(266, 268, 'не'),\n",
       " Substring(269, 277, 'ославили'),\n",
       " Substring(278, 280, 'бы'),\n",
       " Substring(281, 297, 'коммунистической'),\n",
       " Substring(297, 298, '?'),\n",
       " Substring(299, 302, 'Где'),\n",
       " Substring(303, 305, 'та'),\n",
       " Substring(306, 319, 'оппозиционная'),\n",
       " Substring(320, 326, 'партия'),\n",
       " Substring(326, 327, ','),\n",
       " Substring(328, 335, 'которая'),\n",
       " Substring(336, 337, 'в'),\n",
       " Substring(338, 342, 'свою'),\n",
       " Substring(343, 350, 'очередь'),\n",
       " Substring(351, 353, 'не'),\n",
       " Substring(354, 361, 'бросала'),\n",
       " Substring(362, 364, 'бы'),\n",
       " Substring(365, 375, 'клеймящего'),\n",
       " Substring(376, 385, 'обвинения'),\n",
       " Substring(386, 387, 'в'),\n",
       " Substring(388, 398, 'коммунизме'),\n",
       " Substring(399, 402, 'как'),\n",
       " Substring(403, 408, 'более'),\n",
       " Substring(409, 418, 'передовым'),\n",
       " Substring(419, 433, 'представителям'),\n",
       " Substring(434, 443, 'оппозиции'),\n",
       " Substring(443, 444, ','),\n",
       " Substring(445, 448, 'так'),\n",
       " Substring(449, 450, 'и'),\n",
       " Substring(451, 456, 'своим'),\n",
       " Substring(457, 468, 'реакционным'),\n",
       " Substring(469, 480, 'противникам'),\n",
       " Substring(480, 481, '?'),\n",
       " Substring(482, 485, 'Два'),\n",
       " Substring(486, 492, 'вывода'),\n",
       " Substring(493, 501, 'вытекают'),\n",
       " Substring(502, 504, 'из'),\n",
       " Substring(505, 510, 'этого'),\n",
       " Substring(511, 516, 'факта'),\n",
       " Substring(516, 517, '.'),\n",
       " Substring(518, 527, 'Коммунизм'),\n",
       " Substring(528, 538, 'признается'),\n",
       " Substring(539, 542, 'уже'),\n",
       " Substring(543, 548, 'силой'),\n",
       " Substring(549, 554, 'всеми'),\n",
       " Substring(555, 567, 'европейскими'),\n",
       " Substring(568, 574, 'силами'),\n",
       " Substring(574, 575, '.'),\n",
       " Substring(576, 580, 'Пора'),\n",
       " Substring(581, 584, 'уже'),\n",
       " Substring(585, 596, 'коммунистам'),\n",
       " Substring(597, 602, 'перед'),\n",
       " Substring(603, 607, 'всем'),\n",
       " Substring(608, 613, 'миром'),\n",
       " Substring(614, 621, 'открыто'),\n",
       " Substring(622, 630, 'изложить'),\n",
       " Substring(631, 635, 'свои'),\n",
       " Substring(636, 643, 'взгляды'),\n",
       " Substring(643, 644, ','),\n",
       " Substring(645, 649, 'свои'),\n",
       " Substring(650, 654, 'цели'),\n",
       " Substring(654, 655, ','),\n",
       " Substring(656, 660, 'свои'),\n",
       " Substring(661, 671, 'стремления'),\n",
       " Substring(672, 673, 'и'),\n",
       " Substring(674, 681, 'сказкам'),\n",
       " Substring(682, 683, 'о'),\n",
       " Substring(684, 692, 'призраке'),\n",
       " Substring(693, 703, 'коммунизма'),\n",
       " Substring(704, 720, 'противопоставить'),\n",
       " Substring(721, 729, 'манифест'),\n",
       " Substring(730, 735, 'самой'),\n",
       " Substring(736, 742, 'партии'),\n",
       " Substring(742, 743, '.'),\n",
       " Substring(744, 745, 'С'),\n",
       " Substring(746, 750, 'этой'),\n",
       " Substring(751, 756, 'целью'),\n",
       " Substring(757, 758, 'в'),\n",
       " Substring(759, 766, 'Лондоне'),\n",
       " Substring(767, 776, 'собрались'),\n",
       " Substring(777, 787, 'коммунисты'),\n",
       " Substring(788, 793, 'самых'),\n",
       " Substring(794, 803, 'различных'),\n",
       " Substring(804, 819, 'национальностей'),\n",
       " Substring(820, 821, 'и'),\n",
       " Substring(822, 831, 'составили'),\n",
       " Substring(832, 841, 'следующий'),\n",
       " Substring(842, 843, '\"'),\n",
       " Substring(843, 851, 'Манифест'),\n",
       " Substring(851, 852, '\"'),\n",
       " Substring(852, 853, ','),\n",
       " Substring(854, 861, 'который'),\n",
       " Substring(862, 873, 'публикуется'),\n",
       " Substring(874, 876, 'на'),\n",
       " Substring(877, 887, 'английском'),\n",
       " Substring(887, 888, ','),\n",
       " Substring(889, 900, 'французском'),\n",
       " Substring(900, 901, ','),\n",
       " Substring(902, 910, 'немецком'),\n",
       " Substring(910, 911, ','),\n",
       " Substring(912, 923, 'итальянском'),\n",
       " Substring(923, 924, ','),\n",
       " Substring(925, 936, 'фламандском'),\n",
       " Substring(937, 938, 'и'),\n",
       " Substring(939, 946, 'датском'),\n",
       " Substring(947, 953, 'языках'),\n",
       " Substring(953, 954, '.')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3 = '''Призрак бродит по Европе - призрак коммунизма. Все силы старой Европы объединились для священной травли этого призрака: папа и царь, Меттерних и Гизо, французские радикалы и немецкие полицейские.\n",
    "Где та оппозиционная партия, которую ее противники, стоящие у власти, не ославили бы коммунистической? Где та оппозиционная партия, которая в свою очередь не бросала бы клеймящего обвинения в коммунизме как более передовым представителям оппозиции, так и своим реакционным противникам?\n",
    "Два вывода вытекают из этого факта.\n",
    "Коммунизм признается уже силой всеми европейскими силами.\n",
    "Пора уже коммунистам перед всем миром открыто изложить свои взгляды, свои цели, свои стремления и сказкам о призраке коммунизма противопоставить манифест самой партии.\n",
    "С этой целью в Лондоне собрались коммунисты самых различных национальностей и составили следующий \"Манифест\", который публикуется на английском, французском, немецком, итальянском, фламандском и датском языках.'''\n",
    "\n",
    "list(razdel_tokenize(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': [{'lex': 'призрак', 'wt': 1, 'gr': 'S,муж=(им,ед,од|вин,ед,неод|им,ед,неод)'}], 'text': 'Призрак'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'бродить', 'wt': 1, 'gr': 'V,несов,нп=непрош,ед,изъяв,3-л'}], 'text': 'бродит'} \n",
      "\n",
      "Призрак\n",
      " \n",
      "бродит\n",
      " \n",
      "по\n",
      " \n",
      "Европе\n",
      " - \n",
      "призрак\n",
      " \n",
      "коммунизма\n",
      ". \n",
      "Все\n",
      " \n",
      "силы\n",
      " \n",
      "старой\n",
      " \n",
      "Европы\n",
      " \n",
      "объединились\n",
      " \n",
      "для\n",
      " \n",
      "священной\n",
      " \n",
      "травли\n",
      " \n",
      "этого\n",
      " \n",
      "призрака\n",
      ": \n",
      "папа\n",
      " \n",
      "и\n",
      " \n",
      "царь\n",
      ", \n",
      "Меттерних\n",
      " \n",
      "и\n",
      " \n",
      "Гизо\n",
      ", \n",
      "французские\n",
      " \n",
      "радикалы\n",
      " \n",
      "и\n",
      " \n",
      "немецкие\n",
      " \n",
      "полицейские\n",
      ".\n",
      "\n",
      "\n",
      "Где\n",
      " \n",
      "та\n",
      " \n",
      "оппозиционная\n",
      " \n",
      "партия\n",
      ", \n",
      "которую\n",
      " \n",
      "ее\n",
      " \n",
      "противники\n",
      ", \n",
      "стоящие\n",
      " \n",
      "у\n",
      " \n",
      "власти\n",
      ", \n",
      "не\n",
      " \n",
      "ославили\n",
      " \n",
      "бы\n",
      " \n",
      "коммунистической\n",
      "? \n",
      "Где\n",
      " \n",
      "та\n",
      " \n",
      "оппозиционная\n",
      " \n",
      "партия\n",
      ", \n",
      "которая\n",
      " \n",
      "в\n",
      " \n",
      "свою\n",
      " \n",
      "очередь\n",
      " \n",
      "не\n",
      " \n",
      "бросала\n",
      " \n",
      "бы\n",
      " \n",
      "клеймящего\n",
      " \n",
      "обвинения\n",
      " \n",
      "в\n",
      " \n",
      "коммунизме\n",
      " \n",
      "как\n",
      " \n",
      "более\n",
      " \n",
      "передовым\n",
      " \n",
      "представителям\n",
      " \n",
      "оппозиции\n",
      ", \n",
      "так\n",
      " \n",
      "и\n",
      " \n",
      "своим\n",
      " \n",
      "реакционным\n",
      " \n",
      "противникам\n",
      "?\n",
      "\n",
      "\n",
      "Два\n",
      " \n",
      "вывода\n",
      " \n",
      "вытекают\n",
      " \n",
      "из\n",
      " \n",
      "этого\n",
      " \n",
      "факта\n",
      ".\n",
      "\n",
      "\n",
      "Коммунизм\n",
      " \n",
      "признается\n",
      " \n",
      "уже\n",
      " \n",
      "силой\n",
      " \n",
      "всеми\n",
      " \n",
      "европейскими\n",
      " \n",
      "силами\n",
      ".\n",
      "\n",
      "\n",
      "Пора\n",
      " \n",
      "уже\n",
      " \n",
      "коммунистам\n",
      " \n",
      "перед\n",
      " \n",
      "всем\n",
      " \n",
      "миром\n",
      " \n",
      "открыто\n",
      " \n",
      "изложить\n",
      " \n",
      "свои\n",
      " \n",
      "взгляды\n",
      ", \n",
      "свои\n",
      " \n",
      "цели\n",
      ", \n",
      "свои\n",
      " \n",
      "стремления\n",
      " \n",
      "и\n",
      " \n",
      "сказкам\n",
      " \n",
      "о\n",
      " \n",
      "призраке\n",
      " \n",
      "коммунизма\n",
      " \n",
      "противопоставить\n",
      " \n",
      "манифест\n",
      " \n",
      "самой\n",
      " \n",
      "партии\n",
      ".\n",
      "\n",
      "\n",
      "С\n",
      " \n",
      "этой\n",
      " \n",
      "целью\n",
      " \n",
      "в\n",
      " \n",
      "Лондоне\n",
      " \n",
      "собрались\n",
      " \n",
      "коммунисты\n",
      " \n",
      "самых\n",
      " \n",
      "различных\n",
      " \n",
      "национальностей\n",
      " \n",
      "и\n",
      " \n",
      "составили\n",
      " \n",
      "следующий\n",
      " \"\n",
      "Манифест\n",
      "\", \n",
      "который\n",
      " \n",
      "публикуется\n",
      " \n",
      "на\n",
      " \n",
      "английском\n",
      ", \n",
      "французском\n",
      ", \n",
      "немецком\n",
      ", \n",
      "итальянском\n",
      ", \n",
      "фламандском\n",
      " \n",
      "и\n",
      " \n",
      "датском\n",
      " \n",
      "языках\n",
      ".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_mystem = mystem.analyze(text3)\n",
    "print('\\n'.join(str(i) for i in tokens_mystem[:3]), '\\n')\n",
    "\n",
    "for token in tokens_mystem:\n",
    "    print(token['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Лемматизация Mystem vs Pymorphy\n",
    "\n",
    "Лемматизируйте текст с помощью mystem и pymorphy.\n",
    "Найдите различия в лемматизации.\n",
    "Что, по вашему мнению, работает лучше на этом тексте?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "призрак - призрак --- Parse(word='призрак', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='призрак', score=0.75, methods_stack=((DictionaryAnalyzer(), 'призрак', 2, 0),)) \n",
      "\n",
      "бродит - бродить --- Parse(word='бродит', tag=OpencorporaTag('VERB,impf,intr sing,3per,pres,indc'), normal_form='бродить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'бродит', 498, 5),)) \n",
      "\n",
      "по - по --- Parse(word='по', tag=OpencorporaTag('PREP'), normal_form='по', score=0.998387, methods_stack=((DictionaryAnalyzer(), 'по', 24, 0),)) \n",
      "\n",
      "европе - европа --- Parse(word='европе', tag=OpencorporaTag('NOUN,inan,femn,Sgtm,Geox sing,loct'), normal_form='европа', score=0.892086, methods_stack=((DictionaryAnalyzer(), 'европе', 36, 6),)) \n",
      "\n",
      " -  -  -  --- Parse(word=' - ', tag=OpencorporaTag('PNCT'), normal_form=' - ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ' - '),)) \n",
      "\n",
      "призрак - призрак --- Parse(word='призрак', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='призрак', score=0.75, methods_stack=((DictionaryAnalyzer(), 'призрак', 2, 0),)) \n",
      "\n",
      "коммунизма - коммунизм --- Parse(word='коммунизма', tag=OpencorporaTag('NOUN,inan,masc sing,gent'), normal_form='коммунизм', score=1.0, methods_stack=((DictionaryAnalyzer(), 'коммунизма', 34, 1),)) \n",
      "\n",
      ".  - .  --- Parse(word='. ', tag=OpencorporaTag('PNCT'), normal_form='. ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), '. '),)) \n",
      "\n",
      "всё - всё --- Parse(word='всё', tag=OpencorporaTag('PRCL'), normal_form='всё', score=0.979166, methods_stack=((DictionaryAnalyzer(), 'всё', 22, 0),)) \n",
      "\n",
      "силы - сила --- Parse(word='силы', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сила', score=0.584821, methods_stack=((DictionaryAnalyzer(), 'силы', 55, 1),)) \n",
      "\n",
      "старой - старый --- Parse(word='старой', tag=OpencorporaTag('ADJF,Qual femn,sing,gent'), normal_form='старый', score=0.5625, methods_stack=((DictionaryAnalyzer(), 'старой', 2944, 8),)) \n",
      "\n",
      "европы - европа --- Parse(word='европы', tag=OpencorporaTag('NOUN,inan,femn,Sgtm,Geox sing,gent'), normal_form='европа', score=1.0, methods_stack=((DictionaryAnalyzer(), 'европы', 36, 1),)) \n",
      "\n",
      "объединились - объединиться --- Parse(word='объединились', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='объединиться', score=1.0, methods_stack=((DictionaryAnalyzer(), 'объединились', 652, 4),)) \n",
      "\n",
      "для - для --- Parse(word='для', tag=OpencorporaTag('PREP'), normal_form='для', score=0.999843, methods_stack=((DictionaryAnalyzer(), 'для', 24, 0),)) \n",
      "\n",
      "священной - священный --- Parse(word='священной', tag=OpencorporaTag('ADJF,Qual femn,sing,gent'), normal_form='священный', score=0.375, methods_stack=((DictionaryAnalyzer(), 'священной', 516, 8),)) \n",
      "\n",
      "травли - травля --- Parse(word='травли', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='травля', score=0.666666, methods_stack=((DictionaryAnalyzer(), 'травли', 640, 1),)) \n",
      "\n",
      "этого - это --- Parse(word='этого', tag=OpencorporaTag('NPRO,neut sing,gent'), normal_form='это', score=0.729, methods_stack=((DictionaryAnalyzer(), 'этого', 3240, 1),)) \n",
      "\n",
      "призрака - призрак --- Parse(word='призрака', tag=OpencorporaTag('NOUN,anim,masc sing,gent'), normal_form='призрак', score=0.428571, methods_stack=((DictionaryAnalyzer(), 'призрака', 2, 1),)) \n",
      "\n",
      ":  - :  --- Parse(word=': ', tag=OpencorporaTag('PNCT'), normal_form=': ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ': '),)) \n",
      "\n",
      "папа - папа --- Parse(word='папа', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='папа', score=1.0, methods_stack=((DictionaryAnalyzer(), 'папа', 2380, 0),)) \n",
      "\n",
      "и - и --- Parse(word='и', tag=OpencorporaTag('CONJ'), normal_form='и', score=0.998263, methods_stack=((DictionaryAnalyzer(), 'и', 20, 0),)) \n",
      "\n",
      "царь - царь --- Parse(word='царь', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='царь', score=1.0, methods_stack=((DictionaryAnalyzer(), 'царь', 372, 0),)) \n",
      "\n",
      ",  - ,  --- Parse(word=', ', tag=OpencorporaTag('PNCT'), normal_form=', ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ', '),)) \n",
      "\n",
      "меттерних - меттерний --- Parse(word='меттерних', tag=OpencorporaTag('ADJF plur,gent'), normal_form='меттерний', score=0.3333333333333333, methods_stack=((FakeDictionary(), 'меттерних', 493, 21), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), 'ерних'))) \n",
      "\n",
      "и - и --- Parse(word='и', tag=OpencorporaTag('CONJ'), normal_form='и', score=0.998263, methods_stack=((DictionaryAnalyzer(), 'и', 20, 0),)) \n",
      "\n",
      "гизо - гизый --- Parse(word='гизо', tag=OpencorporaTag('ADJS,Qual neut,sing'), normal_form='гизый', score=0.027027027027027046, methods_stack=((FakeDictionary(), 'гизо', 4, 29), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), 'изо'))) \n",
      "\n",
      ",  - ,  --- Parse(word=', ', tag=OpencorporaTag('PNCT'), normal_form=', ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ', '),)) \n",
      "\n",
      "французские - французский --- Parse(word='французские', tag=OpencorporaTag('ADJF plur,nomn'), normal_form='французский', score=0.833333, methods_stack=((DictionaryAnalyzer(), 'французские', 16, 20),)) \n",
      "\n",
      "радикалы - радикал --- Parse(word='радикалы', tag=OpencorporaTag('NOUN,anim,masc plur,nomn'), normal_form='радикал', score=0.625, methods_stack=((DictionaryAnalyzer(), 'радикалы', 52, 6),)) \n",
      "\n",
      "и - и --- Parse(word='и', tag=OpencorporaTag('CONJ'), normal_form='и', score=0.998263, methods_stack=((DictionaryAnalyzer(), 'и', 20, 0),)) \n",
      "\n",
      "немецкие - немецкий --- Parse(word='немецкие', tag=OpencorporaTag('ADJF plur,nomn'), normal_form='немецкий', score=0.857142, methods_stack=((DictionaryAnalyzer(), 'немецкие', 16, 20),)) \n",
      "\n",
      "полицейские - полицейский --- Parse(word='полицейские', tag=OpencorporaTag('NOUN,anim,masc plur,nomn'), normal_form='полицейский', score=0.9, methods_stack=((DictionaryAnalyzer(), 'полицейские', 481, 6),)) \n",
      "\n",
      ". - . --- Parse(word='.', tag=OpencorporaTag('PNCT'), normal_form='.', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), '.'),)) \n",
      "\n",
      "где - где --- Parse(word='где', tag=OpencorporaTag('ADVB,Ques'), normal_form='где', score=0.636363, methods_stack=((DictionaryAnalyzer(), 'где', 1181, 0),)) \n",
      "\n",
      "та - тот --- Parse(word='та', tag=OpencorporaTag('ADJF,Subx,Apro,Anph femn,sing,nomn'), normal_form='тот', score=1.0, methods_stack=((DictionaryAnalyzer(), 'та', 3027, 7),)) \n",
      "\n",
      "оппозиционная - оппозиционный --- Parse(word='оппозиционная', tag=OpencorporaTag('ADJF femn,sing,nomn'), normal_form='оппозиционный', score=1.0, methods_stack=((DictionaryAnalyzer(), 'оппозиционная', 35, 7),)) \n",
      "\n",
      "партия - партия --- Parse(word='партия', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='партия', score=1.0, methods_stack=((DictionaryAnalyzer(), 'партия', 41, 0),)) \n",
      "\n",
      ",  - ,  --- Parse(word=', ', tag=OpencorporaTag('PNCT'), normal_form=', ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ', '),)) \n",
      "\n",
      "которую - который --- Parse(word='которую', tag=OpencorporaTag('ADJF,Subx,Apro,Anph femn,sing,accs'), normal_form='который', score=1.0, methods_stack=((DictionaryAnalyzer(), 'которую', 1867, 10),)) \n",
      "\n",
      "её - она --- Parse(word='её', tag=OpencorporaTag('NPRO,femn,3per,Anph sing,accs'), normal_form='она', score=0.103448, methods_stack=((DictionaryAnalyzer(), 'её', 2305, 6),)) \n",
      "\n",
      "противники - противник --- Parse(word='противники', tag=OpencorporaTag('NOUN,anim,masc plur,nomn'), normal_form='противник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'противники', 2, 6),)) \n",
      "\n",
      ",  - ,  --- Parse(word=', ', tag=OpencorporaTag('PNCT'), normal_form=', ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ', '),)) \n",
      "\n",
      "стоящие - стоящий --- Parse(word='стоящие', tag=OpencorporaTag('ADJF plur,nomn'), normal_form='стоящий', score=0.16666666666666666, methods_stack=((DictionaryAnalyzer(), 'стоящие', 165, 20),)) \n",
      "\n",
      "у - у --- Parse(word='у', tag=OpencorporaTag('PREP'), normal_form='у', score=0.9959, methods_stack=((DictionaryAnalyzer(), 'у', 24, 0),)) \n",
      "\n",
      "власти - власть --- Parse(word='власти', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='власть', score=0.573012, methods_stack=((DictionaryAnalyzer(), 'власти', 13, 1),)) \n",
      "\n",
      ",  - ,  --- Parse(word=', ', tag=OpencorporaTag('PNCT'), normal_form=', ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ', '),)) \n",
      "\n",
      "не - не --- Parse(word='не', tag=OpencorporaTag('PRCL'), normal_form='не', score=1.0, methods_stack=((DictionaryAnalyzer(), 'не', 22, 0),)) \n",
      "\n",
      "ославили - ославить --- Parse(word='ославили', tag=OpencorporaTag('VERB,perf,tran plur,past,indc'), normal_form='ославить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'ославили', 787, 4),)) \n",
      "\n",
      "бы - бы --- Parse(word='бы', tag=OpencorporaTag('PRCL'), normal_form='бы', score=1.0, methods_stack=((DictionaryAnalyzer(), 'бы', 22, 0),)) \n",
      "\n",
      "коммунистической - коммунистический --- Parse(word='коммунистической', tag=OpencorporaTag('ADJF femn,sing,gent'), normal_form='коммунистический', score=0.785714, methods_stack=((DictionaryAnalyzer(), 'коммунистической', 16, 8),)) \n",
      "\n",
      "?  - ?  --- Parse(word='? ', tag=OpencorporaTag('PNCT'), normal_form='? ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), '? '),)) \n",
      "\n",
      "где - где --- Parse(word='где', tag=OpencorporaTag('ADVB,Ques'), normal_form='где', score=0.636363, methods_stack=((DictionaryAnalyzer(), 'где', 1181, 0),)) \n",
      "\n",
      "та - тот --- Parse(word='та', tag=OpencorporaTag('ADJF,Subx,Apro,Anph femn,sing,nomn'), normal_form='тот', score=1.0, methods_stack=((DictionaryAnalyzer(), 'та', 3027, 7),)) \n",
      "\n",
      "оппозиционная - оппозиционный --- Parse(word='оппозиционная', tag=OpencorporaTag('ADJF femn,sing,nomn'), normal_form='оппозиционный', score=1.0, methods_stack=((DictionaryAnalyzer(), 'оппозиционная', 35, 7),)) \n",
      "\n",
      "партия - партия --- Parse(word='партия', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='партия', score=1.0, methods_stack=((DictionaryAnalyzer(), 'партия', 41, 0),)) \n",
      "\n",
      ",  - ,  --- Parse(word=', ', tag=OpencorporaTag('PNCT'), normal_form=', ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ', '),)) \n",
      "\n",
      "которая - который --- Parse(word='которая', tag=OpencorporaTag('ADJF,Subx,Apro,Anph femn,sing,nomn'), normal_form='который', score=1.0, methods_stack=((DictionaryAnalyzer(), 'которая', 1867, 7),)) \n",
      "\n",
      "в - в --- Parse(word='в', tag=OpencorporaTag('PREP'), normal_form='в', score=0.999327, methods_stack=((DictionaryAnalyzer(), 'в', 393, 0),)) \n",
      "\n",
      "свою - свой --- Parse(word='свою', tag=OpencorporaTag('ADJF,Apro,Anph femn,sing,accs'), normal_form='свой', score=1.0, methods_stack=((DictionaryAnalyzer(), 'свою', 2851, 10),)) \n",
      "\n",
      "очередь - очередь --- Parse(word='очередь', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form='очередь', score=0.920863, methods_stack=((DictionaryAnalyzer(), 'очередь', 13, 3),)) \n",
      "\n",
      "не - не --- Parse(word='не', tag=OpencorporaTag('PRCL'), normal_form='не', score=1.0, methods_stack=((DictionaryAnalyzer(), 'не', 22, 0),)) \n",
      "\n",
      "бросала - бросать --- Parse(word='бросала', tag=OpencorporaTag('VERB,impf,tran femn,sing,past,indc'), normal_form='бросать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'бросала', 215, 8),)) \n",
      "\n",
      "бы - бы --- Parse(word='бы', tag=OpencorporaTag('PRCL'), normal_form='бы', score=1.0, methods_stack=((DictionaryAnalyzer(), 'бы', 22, 0),)) \n",
      "\n",
      "клеймящего - клеймить --- Parse(word='клеймящего', tag=OpencorporaTag('PRTF,impf,tran,pres,actv masc,sing,gent'), normal_form='клеймить', score=0.3333333333333333, methods_stack=((DictionaryAnalyzer(), 'клеймящего', 1795, 14),)) \n",
      "\n",
      "обвинения - обвинение --- Parse(word='обвинения', tag=OpencorporaTag('NOUN,inan,neut plur,accs'), normal_form='обвинение', score=0.416666, methods_stack=((DictionaryAnalyzer(), 'обвинения', 77, 18),)) \n",
      "\n",
      "в - в --- Parse(word='в', tag=OpencorporaTag('PREP'), normal_form='в', score=0.999327, methods_stack=((DictionaryAnalyzer(), 'в', 393, 0),)) \n",
      "\n",
      "коммунизме - коммунизм --- Parse(word='коммунизме', tag=OpencorporaTag('NOUN,inan,masc sing,loct'), normal_form='коммунизм', score=1.0, methods_stack=((DictionaryAnalyzer(), 'коммунизме', 34, 5),)) \n",
      "\n",
      "как - как --- Parse(word='как', tag=OpencorporaTag('CONJ'), normal_form='как', score=0.827586, methods_stack=((DictionaryAnalyzer(), 'как', 1751, 0),)) \n",
      "\n",
      "более - более --- Parse(word='более', tag=OpencorporaTag('ADVB'), normal_form='более', score=1.0, methods_stack=((DictionaryAnalyzer(), 'более', 3, 0),)) \n",
      "\n",
      "передовым - передовой --- Parse(word='передовым', tag=OpencorporaTag('ADJF masc,sing,ablt'), normal_form='передовой', score=0.3333333333333333, methods_stack=((DictionaryAnalyzer(), 'передовым', 1680, 5),)) \n",
      "\n",
      "представителям - представитель --- Parse(word='представителям', tag=OpencorporaTag('NOUN,anim,masc plur,datv'), normal_form='представитель', score=1.0, methods_stack=((DictionaryAnalyzer(), 'представителям', 123, 8),)) \n",
      "\n",
      "оппозиции - оппозиция --- Parse(word='оппозиции', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='оппозиция', score=0.838235, methods_stack=((DictionaryAnalyzer(), 'оппозиции', 41, 1),)) \n",
      "\n",
      ",  - ,  --- Parse(word=', ', tag=OpencorporaTag('PNCT'), normal_form=', ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ', '),)) \n",
      "\n",
      "так - так --- Parse(word='так', tag=OpencorporaTag('CONJ'), normal_form='так', score=0.5, methods_stack=((DictionaryAnalyzer(), 'так', 2997, 0),)) \n",
      "\n",
      "и - и --- Parse(word='и', tag=OpencorporaTag('CONJ'), normal_form='и', score=0.998263, methods_stack=((DictionaryAnalyzer(), 'и', 20, 0),)) \n",
      "\n",
      "своим - свой --- Parse(word='своим', tag=OpencorporaTag('ADJF,Apro,Anph plur,datv'), normal_form='свой', score=0.460264, methods_stack=((DictionaryAnalyzer(), 'своим', 2851, 22),)) \n",
      "\n",
      "реакционным - реакционный --- Parse(word='реакционным', tag=OpencorporaTag('ADJF,Qual masc,sing,ablt'), normal_form='реакционный', score=0.3333333333333333, methods_stack=((DictionaryAnalyzer(), 'реакционным', 12, 5),)) \n",
      "\n",
      "противникам - противник --- Parse(word='противникам', tag=OpencorporaTag('NOUN,anim,masc plur,datv'), normal_form='противник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'противникам', 2, 8),)) \n",
      "\n",
      "? - ? --- Parse(word='?', tag=OpencorporaTag('PNCT'), normal_form='?', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), '?'),)) \n",
      "\n",
      "два - два --- Parse(word='два', tag=OpencorporaTag('NUMR masc,nomn'), normal_form='два', score=0.4, methods_stack=((DictionaryAnalyzer(), 'два', 1308, 0),)) \n",
      "\n",
      "вывода - вывод --- Parse(word='вывода', tag=OpencorporaTag('NOUN,inan,masc sing,gent'), normal_form='вывод', score=1.0, methods_stack=((DictionaryAnalyzer(), 'вывода', 34, 1),)) \n",
      "\n",
      "вытекают - вытекать --- Parse(word='вытекают', tag=OpencorporaTag('VERB,impf,intr plur,3per,pres,indc'), normal_form='вытекать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'вытекают', 15, 6),)) \n",
      "\n",
      "из - из --- Parse(word='из', tag=OpencorporaTag('PREP'), normal_form='из', score=0.999673, methods_stack=((DictionaryAnalyzer(), 'из', 393, 0),)) \n",
      "\n",
      "этого - это --- Parse(word='этого', tag=OpencorporaTag('NPRO,neut sing,gent'), normal_form='это', score=0.729, methods_stack=((DictionaryAnalyzer(), 'этого', 3240, 1),)) \n",
      "\n",
      "факта - факт --- Parse(word='факта', tag=OpencorporaTag('NOUN,inan,masc sing,gent'), normal_form='факт', score=1.0, methods_stack=((DictionaryAnalyzer(), 'факта', 34, 1),)) \n",
      "\n",
      ". - . --- Parse(word='.', tag=OpencorporaTag('PNCT'), normal_form='.', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), '.'),)) \n",
      "\n",
      "коммунизм - коммунизм --- Parse(word='коммунизм', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='коммунизм', score=0.6, methods_stack=((DictionaryAnalyzer(), 'коммунизм', 34, 3),)) \n",
      "\n",
      "признаётся - признаваться --- Parse(word='признаётся', tag=OpencorporaTag('VERB,impf,intr sing,3per,pres,indc'), normal_form='признаваться', score=0.8, methods_stack=((DictionaryAnalyzer(), 'признаётся', 679, 5),)) \n",
      "\n",
      "уже - уже --- Parse(word='уже', tag=OpencorporaTag('ADVB'), normal_form='уже', score=0.692307, methods_stack=((DictionaryAnalyzer(), 'уже', 3, 0),)) \n",
      "\n",
      "силой - сила --- Parse(word='силой', tag=OpencorporaTag('NOUN,inan,femn sing,ablt'), normal_form='сила', score=0.870967, methods_stack=((DictionaryAnalyzer(), 'силой', 55, 4),)) \n",
      "\n",
      "всеми - весь --- Parse(word='всеми', tag=OpencorporaTag('ADJF,Subx,Apro plur,ablt'), normal_form='весь', score=1.0, methods_stack=((DictionaryAnalyzer(), 'всеми', 734, 25),)) \n",
      "\n",
      "европейскими - европейский --- Parse(word='европейскими', tag=OpencorporaTag('ADJF plur,ablt'), normal_form='европейский', score=1.0, methods_stack=((DictionaryAnalyzer(), 'европейскими', 16, 25),)) \n",
      "\n",
      "силами - сила --- Parse(word='силами', tag=OpencorporaTag('NOUN,inan,femn plur,ablt'), normal_form='сила', score=1.0, methods_stack=((DictionaryAnalyzer(), 'силами', 55, 11),)) \n",
      "\n",
      ". - . --- Parse(word='.', tag=OpencorporaTag('PNCT'), normal_form='.', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), '.'),)) \n",
      "\n",
      "пора - пора --- Parse(word='пора', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='пора', score=0.5, methods_stack=((DictionaryAnalyzer(), 'пора', 55, 0),)) \n",
      "\n",
      "уже - уже --- Parse(word='уже', tag=OpencorporaTag('ADVB'), normal_form='уже', score=0.692307, methods_stack=((DictionaryAnalyzer(), 'уже', 3, 0),)) \n",
      "\n",
      "коммунистам - коммунист --- Parse(word='коммунистам', tag=OpencorporaTag('NOUN,anim,masc plur,datv'), normal_form='коммунист', score=1.0, methods_stack=((DictionaryAnalyzer(), 'коммунистам', 52, 8),)) \n",
      "\n",
      "перед - перед --- Parse(word='перед', tag=OpencorporaTag('PREP'), normal_form='перед', score=0.996992, methods_stack=((DictionaryAnalyzer(), 'перед', 393, 0),)) \n",
      "\n",
      "всем - весь --- Parse(word='всем', tag=OpencorporaTag('ADJF,Subx,Apro plur,datv'), normal_form='весь', score=0.722084, methods_stack=((DictionaryAnalyzer(), 'всем', 734, 22),)) \n",
      "\n",
      "миром - мир --- Parse(word='миром', tag=OpencorporaTag('NOUN,inan,masc sing,ablt'), normal_form='мир', score=0.973684, methods_stack=((DictionaryAnalyzer(), 'миром', 2036, 4),)) \n",
      "\n",
      "открыто - открыто --- Parse(word='открыто', tag=OpencorporaTag('ADVB'), normal_form='открыто', score=0.3333333333333333, methods_stack=((DictionaryAnalyzer(), 'открыто', 3, 0),)) \n",
      "\n",
      "изложить - изложить --- Parse(word='изложить', tag=OpencorporaTag('INFN,perf,tran'), normal_form='изложить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'изложить', 668, 0),)) \n",
      "\n",
      "свои - свой --- Parse(word='свои', tag=OpencorporaTag('ADJF,Apro,Anph inan,plur,accs'), normal_form='свой', score=0.900862, methods_stack=((DictionaryAnalyzer(), 'свои', 2851, 23),)) \n",
      "\n",
      "взгляды - взгляд --- Parse(word='взгляды', tag=OpencorporaTag('NOUN,inan,masc plur,accs'), normal_form='взгляд', score=0.636363, methods_stack=((DictionaryAnalyzer(), 'взгляды', 34, 9),)) \n",
      "\n",
      ",  - ,  --- Parse(word=', ', tag=OpencorporaTag('PNCT'), normal_form=', ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ', '),)) \n",
      "\n",
      "свои - свой --- Parse(word='свои', tag=OpencorporaTag('ADJF,Apro,Anph inan,plur,accs'), normal_form='свой', score=0.900862, methods_stack=((DictionaryAnalyzer(), 'свои', 2851, 23),)) \n",
      "\n",
      "цели - цель --- Parse(word='цели', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='цель', score=0.45614, methods_stack=((DictionaryAnalyzer(), 'цели', 13, 1),)) \n",
      "\n",
      ",  - ,  --- Parse(word=', ', tag=OpencorporaTag('PNCT'), normal_form=', ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ', '),)) \n",
      "\n",
      "свои - свой --- Parse(word='свои', tag=OpencorporaTag('ADJF,Apro,Anph inan,plur,accs'), normal_form='свой', score=0.900862, methods_stack=((DictionaryAnalyzer(), 'свои', 2851, 23),)) \n",
      "\n",
      "стремления - стремление --- Parse(word='стремления', tag=OpencorporaTag('NOUN,inan,neut sing,gent'), normal_form='стремление', score=0.5625, methods_stack=((DictionaryAnalyzer(), 'стремления', 77, 2),)) \n",
      "\n",
      "и - и --- Parse(word='и', tag=OpencorporaTag('CONJ'), normal_form='и', score=0.998263, methods_stack=((DictionaryAnalyzer(), 'и', 20, 0),)) \n",
      "\n",
      "сказкам - сказка --- Parse(word='сказкам', tag=OpencorporaTag('NOUN,inan,femn plur,datv'), normal_form='сказка', score=1.0, methods_stack=((DictionaryAnalyzer(), 'сказкам', 8, 9),)) \n",
      "\n",
      "о - о --- Parse(word='о', tag=OpencorporaTag('PREP'), normal_form='о', score=0.987837, methods_stack=((DictionaryAnalyzer(), 'о', 2226, 0),)) \n",
      "\n",
      "призраке - призрак --- Parse(word='призраке', tag=OpencorporaTag('NOUN,anim,masc sing,loct'), normal_form='призрак', score=0.666666, methods_stack=((DictionaryAnalyzer(), 'призраке', 2, 5),)) \n",
      "\n",
      "коммунизма - коммунизм --- Parse(word='коммунизма', tag=OpencorporaTag('NOUN,inan,masc sing,gent'), normal_form='коммунизм', score=1.0, methods_stack=((DictionaryAnalyzer(), 'коммунизма', 34, 1),)) \n",
      "\n",
      "противопоставить - противопоставить --- Parse(word='противопоставить', tag=OpencorporaTag('INFN,perf,tran'), normal_form='противопоставить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'противопоставить', 787, 0),)) \n",
      "\n",
      "манифест - манифест --- Parse(word='манифест', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='манифест', score=0.6, methods_stack=((DictionaryAnalyzer(), 'манифест', 34, 0),)) \n",
      "\n",
      "самой - сам --- Parse(word='самой', tag=OpencorporaTag('ADJF,Apro femn,sing,gent'), normal_form='сам', score=0.125, methods_stack=((DictionaryAnalyzer(), 'самой', 2824, 8),)) \n",
      "\n",
      "партии - партия --- Parse(word='партии', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='партия', score=0.788944, methods_stack=((DictionaryAnalyzer(), 'партии', 41, 1),)) \n",
      "\n",
      ". - . --- Parse(word='.', tag=OpencorporaTag('PNCT'), normal_form='.', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), '.'),)) \n",
      "\n",
      "с - с --- Parse(word='с', tag=OpencorporaTag('PREP'), normal_form='с', score=0.997625, methods_stack=((DictionaryAnalyzer(), 'с', 393, 0),)) \n",
      "\n",
      "этой - этот --- Parse(word='этой', tag=OpencorporaTag('ADJF,Subx,Apro,Anph femn,sing,gent'), normal_form='этот', score=0.459783, methods_stack=((DictionaryAnalyzer(), 'этой', 3241, 8),)) \n",
      "\n",
      "целью - цель --- Parse(word='целью', tag=OpencorporaTag('NOUN,inan,femn sing,ablt'), normal_form='цель', score=1.0, methods_stack=((DictionaryAnalyzer(), 'целью', 13, 4),)) \n",
      "\n",
      "в - в --- Parse(word='в', tag=OpencorporaTag('PREP'), normal_form='в', score=0.999327, methods_stack=((DictionaryAnalyzer(), 'в', 393, 0),)) \n",
      "\n",
      "лондоне - лондон --- Parse(word='лондоне', tag=OpencorporaTag('NOUN,inan,masc,Geox sing,loct'), normal_form='лондон', score=1.0, methods_stack=((DictionaryAnalyzer(), 'лондоне', 33, 5),)) \n",
      "\n",
      "собрались - собраться --- Parse(word='собрались', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='собраться', score=1.0, methods_stack=((DictionaryAnalyzer(), 'собрались', 1350, 4),)) \n",
      "\n",
      "коммунисты - коммунист --- Parse(word='коммунисты', tag=OpencorporaTag('NOUN,anim,masc plur,nomn'), normal_form='коммунист', score=1.0, methods_stack=((DictionaryAnalyzer(), 'коммунисты', 52, 6),)) \n",
      "\n",
      "самых - самый --- Parse(word='самых', tag=OpencorporaTag('ADJF,Apro plur,gent'), normal_form='самый', score=0.870967, methods_stack=((DictionaryAnalyzer(), 'самых', 2826, 20),)) \n",
      "\n",
      "различных - различный --- Parse(word='различных', tag=OpencorporaTag('ADJF,Qual plur,gent'), normal_form='различный', score=0.725888, methods_stack=((DictionaryAnalyzer(), 'различных', 12, 21),)) \n",
      "\n",
      "национальностей - национальность --- Parse(word='национальностей', tag=OpencorporaTag('NOUN,inan,femn plur,gent'), normal_form='национальность', score=1.0, methods_stack=((DictionaryAnalyzer(), 'национальностей', 13, 7),)) \n",
      "\n",
      "и - и --- Parse(word='и', tag=OpencorporaTag('CONJ'), normal_form='и', score=0.998263, methods_stack=((DictionaryAnalyzer(), 'и', 20, 0),)) \n",
      "\n",
      "составили - составить --- Parse(word='составили', tag=OpencorporaTag('VERB,perf,tran plur,past,indc'), normal_form='составить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'составили', 787, 4),)) \n",
      "\n",
      "следующий - следующий --- Parse(word='следующий', tag=OpencorporaTag('ADJF masc,sing,nomn'), normal_form='следующий', score=0.25, methods_stack=((DictionaryAnalyzer(), 'следующий', 213, 0),)) \n",
      "\n",
      " \" -  \" --- Parse(word=' \"', tag=OpencorporaTag('PNCT'), normal_form=' \"', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ' \"'),)) \n",
      "\n",
      "манифест - манифест --- Parse(word='манифест', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='манифест', score=0.6, methods_stack=((DictionaryAnalyzer(), 'манифест', 34, 0),)) \n",
      "\n",
      "\",  - \",  --- Parse(word='\", ', tag=OpencorporaTag('PNCT'), normal_form='\", ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), '\", '),)) \n",
      "\n",
      "который - который --- Parse(word='который', tag=OpencorporaTag('ADJF,Subx,Apro,Anph masc,sing,nomn'), normal_form='который', score=0.828767, methods_stack=((DictionaryAnalyzer(), 'который', 1867, 0),)) \n",
      "\n",
      "публикуется - публиковаться --- Parse(word='публикуется', tag=OpencorporaTag('VERB,impf,intr sing,3per,pres,indc'), normal_form='публиковаться', score=1.0, methods_stack=((DictionaryAnalyzer(), 'публикуется', 140, 5),)) \n",
      "\n",
      "на - на --- Parse(word='на', tag=OpencorporaTag('PREP'), normal_form='на', score=0.998961, methods_stack=((DictionaryAnalyzer(), 'на', 24, 0),)) \n",
      "\n",
      "английском - английский --- Parse(word='английском', tag=OpencorporaTag('ADJF masc,sing,loct'), normal_form='английский', score=0.962264, methods_stack=((DictionaryAnalyzer(), 'английском', 16, 6),)) \n",
      "\n",
      ",  - ,  --- Parse(word=', ', tag=OpencorporaTag('PNCT'), normal_form=', ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ', '),)) \n",
      "\n",
      "французском - французский --- Parse(word='французском', tag=OpencorporaTag('ADJF masc,sing,loct'), normal_form='французский', score=0.857142, methods_stack=((DictionaryAnalyzer(), 'французском', 16, 6),)) \n",
      "\n",
      ",  - ,  --- Parse(word=', ', tag=OpencorporaTag('PNCT'), normal_form=', ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ', '),)) \n",
      "\n",
      "немецком - немецкий --- Parse(word='немецком', tag=OpencorporaTag('ADJF masc,sing,loct'), normal_form='немецкий', score=0.954545, methods_stack=((DictionaryAnalyzer(), 'немецком', 16, 6),)) \n",
      "\n",
      ",  - ,  --- Parse(word=', ', tag=OpencorporaTag('PNCT'), normal_form=', ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ', '),)) \n",
      "\n",
      "итальянском - итальянский --- Parse(word='итальянском', tag=OpencorporaTag('ADJF masc,sing,loct'), normal_form='итальянский', score=0.666666, methods_stack=((DictionaryAnalyzer(), 'итальянском', 16, 6),)) \n",
      "\n",
      ",  - ,  --- Parse(word=', ', tag=OpencorporaTag('PNCT'), normal_form=', ', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ', '),)) \n",
      "\n",
      "фламандском - фламандский --- Parse(word='фламандском', tag=OpencorporaTag('ADJF masc,sing,loct'), normal_form='фламандский', score=0.5, methods_stack=((DictionaryAnalyzer(), 'фламандском', 16, 6),)) \n",
      "\n",
      "и - и --- Parse(word='и', tag=OpencorporaTag('CONJ'), normal_form='и', score=0.998263, methods_stack=((DictionaryAnalyzer(), 'и', 20, 0),)) \n",
      "\n",
      "датском - датский --- Parse(word='датском', tag=OpencorporaTag('ADJF masc,sing,loct'), normal_form='датский', score=0.5, methods_stack=((DictionaryAnalyzer(), 'датском', 16, 6),)) \n",
      "\n",
      "языках - язык --- Parse(word='языках', tag=OpencorporaTag('NOUN,inan,masc plur,loct'), normal_form='язык', score=0.977272, methods_stack=((DictionaryAnalyzer(), 'языках', 3247, 11),)) \n",
      "\n",
      ". - . --- Parse(word='.', tag=OpencorporaTag('PNCT'), normal_form='.', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), '.'),)) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_analized = [morph.parse(token['text']) for token in tokens_mystem]\n",
    "# Она похожа на analyze в майстеме только возрващает список объектов Parse\n",
    "# Первый в списке - самый вероятный разбор (у каждого есть score)\n",
    "# Информация достается через атрибут (Parse.word - например)\n",
    "# Грамматическая информация хранится в объекте OpencorporaTag и из него удобно доставать\n",
    "# части речи или другие категории\n",
    "for word in words_analized:\n",
    "    if not word[0].word.isspace():\n",
    "        print(word[0].word, '-', word[0].normal_form, '---', word[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Призрак - призрак --- {'analysis': [{'lex': 'призрак', 'wt': 1, 'gr': 'S,муж=(им,ед,од|вин,ед,неод|им,ед,неод)'}], 'text': 'Призрак'}\n",
      "бродит - бродить --- {'analysis': [{'lex': 'бродить', 'wt': 1, 'gr': 'V,несов,нп=непрош,ед,изъяв,3-л'}], 'text': 'бродит'}\n",
      "по - по --- {'analysis': [{'lex': 'по', 'wt': 1, 'gr': 'PR='}], 'text': 'по'}\n",
      "Европе - европа --- {'analysis': [{'lex': 'европа', 'wt': 1, 'gr': 'S,гео,жен,неод=(пр,ед|дат,ед)'}], 'text': 'Европе'}\n",
      " -  --- {'text': ' - '}\n",
      "призрак - призрак --- {'analysis': [{'lex': 'призрак', 'wt': 1, 'gr': 'S,муж=(им,ед,од|вин,ед,неод|им,ед,неод)'}], 'text': 'призрак'}\n",
      "коммунизма - коммунизм --- {'analysis': [{'lex': 'коммунизм', 'wt': 1, 'gr': 'S,муж,неод=род,ед'}], 'text': 'коммунизма'}\n",
      ".  --- {'text': '. '}\n",
      "Все - весь --- {'analysis': [{'lex': 'весь', 'wt': 0.5638720238, 'gr': 'APRO=(им,мн|вин,ед,сред|им,ед,сред|вин,мн,неод)'}], 'text': 'Все'}\n",
      "силы - сила --- {'analysis': [{'lex': 'сила', 'wt': 0.9988700383, 'gr': 'S,жен,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'силы'}\n",
      "старой - старый --- {'analysis': [{'lex': 'старый', 'wt': 1, 'gr': 'A=(пр,ед,полн,жен|дат,ед,полн,жен|род,ед,полн,жен|твор,ед,полн,жен)'}], 'text': 'старой'}\n",
      "Европы - европа --- {'analysis': [{'lex': 'европа', 'wt': 1, 'gr': 'S,гео,жен,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'Европы'}\n",
      "объединились - объединяться --- {'analysis': [{'lex': 'объединяться', 'wt': 1, 'gr': 'V,нп=прош,мн,изъяв,сов'}], 'text': 'объединились'}\n",
      "для - для --- {'analysis': [{'lex': 'для', 'wt': 1, 'gr': 'PR='}], 'text': 'для'}\n",
      "священной - священный --- {'analysis': [{'lex': 'священный', 'wt': 1, 'gr': 'A=(пр,ед,полн,жен|дат,ед,полн,жен|род,ед,полн,жен|твор,ед,полн,жен)'}], 'text': 'священной'}\n",
      "травли - травля --- {'analysis': [{'lex': 'травля', 'wt': 1, 'gr': 'S,жен,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'травли'}\n",
      "этого - этот --- {'analysis': [{'lex': 'этот', 'wt': 0.4614766617, 'gr': 'APRO=(вин,ед,муж,од|род,ед,муж|род,ед,сред)'}], 'text': 'этого'}\n",
      "призрака - призрак --- {'analysis': [{'lex': 'призрак', 'wt': 1, 'gr': 'S,муж=(вин,ед,од|род,ед,од|род,ед,неод)'}], 'text': 'призрака'}\n",
      ":  --- {'text': ': '}\n",
      "папа - папа --- {'analysis': [{'lex': 'папа', 'wt': 0.9996149827, 'gr': 'S,муж,од=им,ед'}], 'text': 'папа'}\n",
      "и - и --- {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'}\n",
      "царь - царь --- {'analysis': [{'lex': 'царь', 'wt': 1, 'gr': 'S,муж,од=им,ед'}], 'text': 'царь'}\n",
      ",  --- {'text': ', '}\n",
      "Меттерних - меттерний --- {'analysis': [{'lex': 'меттерний', 'wt': 1, 'qual': 'bastard', 'gr': 'A=(пр,мн,полн|вин,мн,полн,од|род,мн,полн)'}], 'text': 'Меттерних'}\n",
      "и - и --- {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'}\n",
      "Гизо - гизо --- {'analysis': [{'lex': 'гизо', 'wt': 1, 'gr': 'S,имя,муж,од=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)'}], 'text': 'Гизо'}\n",
      ",  --- {'text': ', '}\n",
      "французские - французский --- {'analysis': [{'lex': 'французский', 'wt': 1, 'gr': 'A=(вин,мн,полн,неод|им,мн,полн)'}], 'text': 'французские'}\n",
      "радикалы - радикал --- {'analysis': [{'lex': 'радикал', 'wt': 0.590212753, 'gr': 'S,муж,од=им,мн'}], 'text': 'радикалы'}\n",
      "и - и --- {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'}\n",
      "немецкие - немецкий --- {'analysis': [{'lex': 'немецкий', 'wt': 1, 'gr': 'A=(вин,мн,полн,неод|им,мн,полн)'}], 'text': 'немецкие'}\n",
      "полицейские - полицейский --- {'analysis': [{'lex': 'полицейский', 'wt': 0.8587260265, 'gr': 'S,муж,од=им,мн'}], 'text': 'полицейские'}\n",
      ". --- {'text': '.'}\n",
      "Где - где --- {'analysis': [{'lex': 'где', 'wt': 1, 'gr': 'ADVPRO='}], 'text': 'Где'}\n",
      "та - тот --- {'analysis': [{'lex': 'тот', 'wt': 1, 'gr': 'APRO=им,ед,жен'}], 'text': 'та'}\n",
      "оппозиционная - оппозиционный --- {'analysis': [{'lex': 'оппозиционный', 'wt': 1, 'gr': 'A=им,ед,полн,жен'}], 'text': 'оппозиционная'}\n",
      "партия - партия --- {'analysis': [{'lex': 'партия', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}], 'text': 'партия'}\n",
      ",  --- {'text': ', '}\n",
      "которую - который --- {'analysis': [{'lex': 'который', 'wt': 1, 'gr': 'APRO=вин,ед,жен'}], 'text': 'которую'}\n",
      "ее - ее --- {'analysis': [{'lex': 'ее', 'wt': 1.234552234e-05, 'gr': 'APRO=(пр,мн|дат,мн|род,мн|твор,мн|им,мн|им,ед,жен|вин,ед,муж,од|род,ед,муж|род,ед,сред|вин,ед,сред|им,ед,сред|пр,ед,жен|дат,ед,жен|род,ед,жен|твор,ед,жен|пр,ед,муж|пр,ед,сред|дат,ед,муж|дат,ед,сред|вин,ед,жен|вин,мн,неод|вин,ед,муж,неод|им,ед,муж|твор,ед,муж|твор,ед,сред|вин,мн,од|пр,мн,сред|вин,мн,сред|дат,мн,сред|род,мн,сред|твор,мн,сред|им,мн,сред|пр,мн,муж|вин,мн,муж|вин,ед,муж|дат,мн,муж)'}], 'text': 'ее'}\n",
      "противники - противник --- {'analysis': [{'lex': 'противник', 'wt': 1, 'gr': 'S,муж,од=им,мн'}], 'text': 'противники'}\n",
      ",  --- {'text': ', '}\n",
      "стоящие - стоять --- {'analysis': [{'lex': 'стоять', 'wt': 0.8676257974, 'gr': 'V,несов,нп=(непрош,им,мн,прич,полн,действ|непрош,вин,мн,прич,полн,действ,неод)'}], 'text': 'стоящие'}\n",
      "у - у --- {'analysis': [{'lex': 'у', 'wt': 0.9993940324, 'gr': 'PR='}], 'text': 'у'}\n",
      "власти - власть --- {'analysis': [{'lex': 'власть', 'wt': 1, 'gr': 'S,жен,неод=(пр,ед|вин,мн|дат,ед|род,ед|им,мн)'}], 'text': 'власти'}\n",
      ",  --- {'text': ', '}\n",
      "не - не --- {'analysis': [{'lex': 'не', 'wt': 1, 'gr': 'PART='}], 'text': 'не'}\n",
      "ославили - ославлять --- {'analysis': [{'lex': 'ославлять', 'wt': 1, 'gr': 'V,пе=прош,мн,изъяв,сов'}], 'text': 'ославили'}\n",
      "бы - бы --- {'analysis': [{'lex': 'бы', 'wt': 1, 'gr': 'PART='}], 'text': 'бы'}\n",
      "коммунистической - коммунистический --- {'analysis': [{'lex': 'коммунистический', 'wt': 1, 'gr': 'A=(пр,ед,полн,жен|дат,ед,полн,жен|род,ед,полн,жен|твор,ед,полн,жен)'}], 'text': 'коммунистической'}\n",
      "?  --- {'text': '? '}\n",
      "Где - где --- {'analysis': [{'lex': 'где', 'wt': 1, 'gr': 'ADVPRO='}], 'text': 'Где'}\n",
      "та - тот --- {'analysis': [{'lex': 'тот', 'wt': 1, 'gr': 'APRO=им,ед,жен'}], 'text': 'та'}\n",
      "оппозиционная - оппозиционный --- {'analysis': [{'lex': 'оппозиционный', 'wt': 1, 'gr': 'A=им,ед,полн,жен'}], 'text': 'оппозиционная'}\n",
      "партия - партия --- {'analysis': [{'lex': 'партия', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}], 'text': 'партия'}\n",
      ",  --- {'text': ', '}\n",
      "которая - который --- {'analysis': [{'lex': 'который', 'wt': 1, 'gr': 'APRO=им,ед,жен'}], 'text': 'которая'}\n",
      "в - в --- {'analysis': [{'lex': 'в', 'wt': 0.9999917878, 'gr': 'PR='}], 'text': 'в'}\n",
      "свою - свой --- {'analysis': [{'lex': 'свой', 'wt': 1, 'gr': 'APRO=вин,ед,жен'}], 'text': 'свою'}\n",
      "очередь - очередь --- {'analysis': [{'lex': 'очередь', 'wt': 1, 'gr': 'S,жен,неод=(вин,ед|им,ед)'}], 'text': 'очередь'}\n",
      "не - не --- {'analysis': [{'lex': 'не', 'wt': 1, 'gr': 'PART='}], 'text': 'не'}\n",
      "бросала - бросать --- {'analysis': [{'lex': 'бросать', 'wt': 1, 'gr': 'V=прош,ед,изъяв,жен,несов,пе'}], 'text': 'бросала'}\n",
      "бы - бы --- {'analysis': [{'lex': 'бы', 'wt': 1, 'gr': 'PART='}], 'text': 'бы'}\n",
      "клеймящего - клеймить --- {'analysis': [{'lex': 'клеймить', 'wt': 1, 'gr': 'V,несов,пе=(непрош,вин,ед,прич,полн,муж,действ,од|непрош,род,ед,прич,полн,муж,действ|непрош,род,ед,прич,полн,сред,действ)'}], 'text': 'клеймящего'}\n",
      "обвинения - обвинение --- {'analysis': [{'lex': 'обвинение', 'wt': 1, 'gr': 'S,сред,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'обвинения'}\n",
      "в - в --- {'analysis': [{'lex': 'в', 'wt': 0.9999917878, 'gr': 'PR='}], 'text': 'в'}\n",
      "коммунизме - коммунизм --- {'analysis': [{'lex': 'коммунизм', 'wt': 1, 'gr': 'S,муж,неод=пр,ед'}], 'text': 'коммунизме'}\n",
      "как - как --- {'analysis': [{'lex': 'как', 'wt': 0.6102574114, 'gr': 'CONJ='}], 'text': 'как'}\n",
      "более - более --- {'analysis': [{'lex': 'более', 'wt': 0.9999468251, 'gr': 'ADV='}], 'text': 'более'}\n",
      "передовым - передовой --- {'analysis': [{'lex': 'передовой', 'wt': 0.9892540811, 'gr': 'A=(дат,мн,полн|твор,ед,полн,муж|твор,ед,полн,сред)'}], 'text': 'передовым'}\n",
      "представителям - представитель --- {'analysis': [{'lex': 'представитель', 'wt': 1, 'gr': 'S,муж,од=дат,мн'}], 'text': 'представителям'}\n",
      "оппозиции - оппозиция --- {'analysis': [{'lex': 'оппозиция', 'wt': 1, 'gr': 'S,жен,неод=(пр,ед|вин,мн|дат,ед|род,ед|им,мн)'}], 'text': 'оппозиции'}\n",
      ",  --- {'text': ', '}\n",
      "так - так --- {'analysis': [{'lex': 'так', 'wt': 0.9840554802, 'gr': 'ADVPRO='}], 'text': 'так'}\n",
      "и - и --- {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'}\n",
      "своим - свой --- {'analysis': [{'lex': 'свой', 'wt': 0.9882044365, 'gr': 'APRO=(дат,мн|твор,ед,муж|твор,ед,сред)'}], 'text': 'своим'}\n",
      "реакционным - реакционный --- {'analysis': [{'lex': 'реакционный', 'wt': 1, 'gr': 'A=(дат,мн,полн|твор,ед,полн,муж|твор,ед,полн,сред)'}], 'text': 'реакционным'}\n",
      "противникам - противник --- {'analysis': [{'lex': 'противник', 'wt': 1, 'gr': 'S,муж,од=дат,мн'}], 'text': 'противникам'}\n",
      "? --- {'text': '?'}\n",
      "Два - два --- {'analysis': [{'lex': 'два', 'wt': 1, 'gr': 'NUM=(вин,муж,неод|им,муж|вин,сред|им,сред)'}], 'text': 'Два'}\n",
      "вывода - вывод --- {'analysis': [{'lex': 'вывод', 'wt': 1, 'gr': 'S,муж,неод=род,ед'}], 'text': 'вывода'}\n",
      "вытекают - вытекать --- {'analysis': [{'lex': 'вытекать', 'wt': 1, 'gr': 'V,нп=непрош,мн,изъяв,3-л,несов'}], 'text': 'вытекают'}\n",
      "из - из --- {'analysis': [{'lex': 'из', 'wt': 0.9999999775, 'gr': 'PR='}], 'text': 'из'}\n",
      "этого - этот --- {'analysis': [{'lex': 'этот', 'wt': 0.4614766617, 'gr': 'APRO=(вин,ед,муж,од|род,ед,муж|род,ед,сред)'}], 'text': 'этого'}\n",
      "факта - факт --- {'analysis': [{'lex': 'факт', 'wt': 1, 'gr': 'S,муж,неод=род,ед'}], 'text': 'факта'}\n",
      ". --- {'text': '.'}\n",
      "Коммунизм - коммунизм --- {'analysis': [{'lex': 'коммунизм', 'wt': 1, 'gr': 'S,муж,неод=(вин,ед|им,ед)'}], 'text': 'Коммунизм'}\n",
      "признается - признаваться --- {'analysis': [{'lex': 'признаваться', 'wt': 1, 'gr': 'V,нп=(непрош,ед,изъяв,3-л,несов|непрош,ед,изъяв,3-л,сов)'}], 'text': 'признается'}\n",
      "уже - уже --- {'analysis': [{'lex': 'уже', 'wt': 0.9985784456, 'gr': 'ADV='}], 'text': 'уже'}\n",
      "силой - сила --- {'analysis': [{'lex': 'сила', 'wt': 0.9933958898, 'gr': 'S,жен,неод=твор,ед'}], 'text': 'силой'}\n",
      "всеми - весь --- {'analysis': [{'lex': 'весь', 'wt': 0.6533386921, 'gr': 'APRO=твор,мн'}], 'text': 'всеми'}\n",
      "европейскими - европейский --- {'analysis': [{'lex': 'европейский', 'wt': 1, 'gr': 'A=твор,мн,полн'}], 'text': 'европейскими'}\n",
      "силами - сила --- {'analysis': [{'lex': 'сила', 'wt': 0.9998377236, 'gr': 'S,жен,неод=твор,мн'}], 'text': 'силами'}\n",
      ". --- {'text': '.'}\n",
      "Пора - пора --- {'analysis': [{'lex': 'пора', 'wt': 0.5541236415, 'gr': 'ADV,прдк='}], 'text': 'Пора'}\n",
      "уже - уже --- {'analysis': [{'lex': 'уже', 'wt': 0.9985784456, 'gr': 'ADV='}], 'text': 'уже'}\n",
      "коммунистам - коммунист --- {'analysis': [{'lex': 'коммунист', 'wt': 1, 'gr': 'S,муж,од=дат,мн'}], 'text': 'коммунистам'}\n",
      "перед - перед --- {'analysis': [{'lex': 'перед', 'wt': 0.9997082151, 'gr': 'PR='}], 'text': 'перед'}\n",
      "всем - весь --- {'analysis': [{'lex': 'весь', 'wt': 0.5549692986, 'gr': 'APRO=(дат,мн|пр,ед,муж|пр,ед,сред|твор,ед,муж|твор,ед,сред)'}], 'text': 'всем'}\n",
      "миром - мир --- {'analysis': [{'lex': 'мир', 'wt': 0.997452298, 'gr': 'S,муж,неод=твор,ед'}], 'text': 'миром'}\n",
      "открыто - открыто --- {'analysis': [{'lex': 'открыто', 'wt': 0.8913990837, 'gr': 'ADV='}], 'text': 'открыто'}\n",
      "изложить - излагать --- {'analysis': [{'lex': 'излагать', 'wt': 1, 'gr': 'V,пе=инф,сов'}], 'text': 'изложить'}\n",
      "свои - свой --- {'analysis': [{'lex': 'свой', 'wt': 0.999988352, 'gr': 'APRO=(им,мн|вин,мн,неод)'}], 'text': 'свои'}\n",
      "взгляды - взгляд --- {'analysis': [{'lex': 'взгляд', 'wt': 1, 'gr': 'S,муж,неод=(вин,мн|им,мн)'}], 'text': 'взгляды'}\n",
      ",  --- {'text': ', '}\n",
      "свои - свой --- {'analysis': [{'lex': 'свой', 'wt': 0.999988352, 'gr': 'APRO=(им,мн|вин,мн,неод)'}], 'text': 'свои'}\n",
      "цели - цель --- {'analysis': [{'lex': 'цель', 'wt': 0.9999471021, 'gr': 'S,жен,неод=(пр,ед|вин,мн|дат,ед|род,ед|им,мн)'}], 'text': 'цели'}\n",
      ",  --- {'text': ', '}\n",
      "свои - свой --- {'analysis': [{'lex': 'свой', 'wt': 0.999988352, 'gr': 'APRO=(им,мн|вин,мн,неод)'}], 'text': 'свои'}\n",
      "стремления - стремление --- {'analysis': [{'lex': 'стремление', 'wt': 1, 'gr': 'S,сред,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'стремления'}\n",
      "и - и --- {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'}\n",
      "сказкам - сказка --- {'analysis': [{'lex': 'сказка', 'wt': 1, 'gr': 'S,жен,неод=дат,мн'}], 'text': 'сказкам'}\n",
      "о - о --- {'analysis': [{'lex': 'о', 'wt': 0.9737875835, 'gr': 'PR='}], 'text': 'о'}\n",
      "призраке - призрак --- {'analysis': [{'lex': 'призрак', 'wt': 1, 'gr': 'S,муж=(пр,ед,од|пр,ед,неод)'}], 'text': 'призраке'}\n",
      "коммунизма - коммунизм --- {'analysis': [{'lex': 'коммунизм', 'wt': 1, 'gr': 'S,муж,неод=род,ед'}], 'text': 'коммунизма'}\n",
      "противопоставить - противопоставлять --- {'analysis': [{'lex': 'противопоставлять', 'wt': 1, 'gr': 'V,пе=инф,сов'}], 'text': 'противопоставить'}\n",
      "манифест - манифест --- {'analysis': [{'lex': 'манифест', 'wt': 1, 'gr': 'S,муж,неод=(вин,ед|им,ед)'}], 'text': 'манифест'}\n",
      "самой - сам --- {'analysis': [{'lex': 'сам', 'wt': 0.2883675665, 'gr': 'APRO=(пр,ед,жен|дат,ед,жен|род,ед,жен|твор,ед,жен)'}], 'text': 'самой'}\n",
      "партии - партия --- {'analysis': [{'lex': 'партия', 'wt': 1, 'gr': 'S,жен,неод=(пр,ед|вин,мн|дат,ед|род,ед|им,мн)'}], 'text': 'партии'}\n",
      ". --- {'text': '.'}\n",
      "С - с --- {'analysis': [{'lex': 'с', 'wt': 0.999977831, 'gr': 'PR='}], 'text': 'С'}\n",
      "этой - этот --- {'analysis': [{'lex': 'этот', 'wt': 0.9999756353, 'gr': 'APRO=(пр,ед,жен|дат,ед,жен|род,ед,жен|твор,ед,жен)'}], 'text': 'этой'}\n",
      "целью - цель --- {'analysis': [{'lex': 'цель', 'wt': 1, 'gr': 'S,жен,неод=твор,ед'}], 'text': 'целью'}\n",
      "в - в --- {'analysis': [{'lex': 'в', 'wt': 0.9999917878, 'gr': 'PR='}], 'text': 'в'}\n",
      "Лондоне - лондон --- {'analysis': [{'lex': 'лондон', 'wt': 1, 'gr': 'S,гео,муж,неод=пр,ед'}], 'text': 'Лондоне'}\n",
      "собрались - собираться --- {'analysis': [{'lex': 'собираться', 'wt': 1, 'gr': 'V,нп=прош,мн,изъяв,сов'}], 'text': 'собрались'}\n",
      "коммунисты - коммунист --- {'analysis': [{'lex': 'коммунист', 'wt': 1, 'gr': 'S,муж,од=им,мн'}], 'text': 'коммунисты'}\n",
      "самых - самый --- {'analysis': [{'lex': 'самый', 'wt': 1, 'gr': 'APRO=(пр,мн|род,мн|вин,мн,од)'}], 'text': 'самых'}\n",
      "различных - различный --- {'analysis': [{'lex': 'различный', 'wt': 1, 'gr': 'A=(пр,мн,полн|вин,мн,полн,од|род,мн,полн)'}], 'text': 'различных'}\n",
      "национальностей - национальность --- {'analysis': [{'lex': 'национальность', 'wt': 1, 'gr': 'S,жен,неод=род,мн'}], 'text': 'национальностей'}\n",
      "и - и --- {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'}\n",
      "составили - составлять --- {'analysis': [{'lex': 'составлять', 'wt': 1, 'gr': 'V,пе=прош,мн,изъяв,сов'}], 'text': 'составили'}\n",
      "следующий - следовать --- {'analysis': [{'lex': 'следовать', 'wt': 0, 'gr': 'V,несов=(непрош,вин,ед,прич,полн,муж,действ,неод|непрош,им,ед,прич,полн,муж,действ)'}], 'text': 'следующий'}\n",
      " \" --- {'text': ' \"'}\n",
      "Манифест - манифест --- {'analysis': [{'lex': 'манифест', 'wt': 1, 'gr': 'S,муж,неод=(вин,ед|им,ед)'}], 'text': 'Манифест'}\n",
      "\",  --- {'text': '\", '}\n",
      "который - который --- {'analysis': [{'lex': 'который', 'wt': 1, 'gr': 'APRO=(вин,ед,муж,неод|им,ед,муж)'}], 'text': 'который'}\n",
      "публикуется - публиковаться --- {'analysis': [{'lex': 'публиковаться', 'wt': 1, 'gr': 'V,несов,нп=непрош,ед,изъяв,3-л'}], 'text': 'публикуется'}\n",
      "на - на --- {'analysis': [{'lex': 'на', 'wt': 0.9989522965, 'gr': 'PR='}], 'text': 'на'}\n",
      "английском - английский --- {'analysis': [{'lex': 'английский', 'wt': 1, 'gr': 'A=(пр,ед,полн,муж|пр,ед,полн,сред)'}], 'text': 'английском'}\n",
      ",  --- {'text': ', '}\n",
      "французском - французский --- {'analysis': [{'lex': 'французский', 'wt': 1, 'gr': 'A=(пр,ед,полн,муж|пр,ед,полн,сред)'}], 'text': 'французском'}\n",
      ",  --- {'text': ', '}\n",
      "немецком - немецкий --- {'analysis': [{'lex': 'немецкий', 'wt': 1, 'gr': 'A=(пр,ед,полн,муж|пр,ед,полн,сред)'}], 'text': 'немецком'}\n",
      ",  --- {'text': ', '}\n",
      "итальянском - итальянский --- {'analysis': [{'lex': 'итальянский', 'wt': 1, 'gr': 'A=(пр,ед,полн,муж|пр,ед,полн,сред)'}], 'text': 'итальянском'}\n",
      ",  --- {'text': ', '}\n",
      "фламандском - фламандский --- {'analysis': [{'lex': 'фламандский', 'wt': 1, 'gr': 'A=(пр,ед,полн,муж|пр,ед,полн,сред)'}], 'text': 'фламандском'}\n",
      "и - и --- {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'}\n",
      "датском - датский --- {'analysis': [{'lex': 'датский', 'wt': 1, 'gr': 'A=(пр,ед,полн,муж|пр,ед,полн,сред)'}], 'text': 'датском'}\n",
      "языках - язык --- {'analysis': [{'lex': 'язык', 'wt': 0.9993644641, 'gr': 'S,муж,неод=пр,мн'}], 'text': 'языках'}\n",
      ". --- {'text': '.'}\n"
     ]
    }
   ],
   "source": [
    "for token in tokens_mystem:\n",
    "    if not token['text'].isspace():\n",
    "        if 'analysis' in token:\n",
    "            print(token['text'], '-', token['analysis'][0]['lex'], '---', token)\n",
    "        else:\n",
    "            print(token['text'], '---', token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
