{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с текстовыми данными обычно начинается с предобработки. Грубо говоря, предобработка - это удаление всего лишнего и приведение к нужному формату. Предобработка может быть долгой и неприятной, но в большинстве случаев все сводится к использованию стандартных инструментов. Эта тетрадка познакомит вас с такими инструментами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Установите все нужные библиотеки. Подробнее про каждую из них ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymystem3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pymystem3) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests->pymystem3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests->pymystem3) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests->pymystem3) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests->pymystem3) (2024.2.2)\n",
      "Collecting pymorphy2\n",
      "  Using cached pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
      "  Using cached DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
      "  Using cached pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting docopt>=0.6 (from pymorphy2)\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Using cached pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "Using cached DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Using cached pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
      "Requirement already satisfied: razdel in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: gensim in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from gensim) (7.0.1)\n",
      "Requirement already satisfied: wrapt in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: nltk in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: rusenttokenize in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (0.0.5)\n",
      "Requirement already satisfied: regex in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (2023.12.25)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymystem3\n",
    "!pip install pymorphy2 # \"pymorphy2[fast]\" doesn't work on macOS arm\n",
    "!pip install razdel\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install rusenttokenize\n",
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скорее всего вы уже знакомы с библиотекой pymorphy2.\n",
    "# pymorphy2[fast] - это оптимизированный pymorphy2, который работает точно также, но сильно быстрее\n",
    "# Если у вас windows, то он вряд ли установится и вам придется пользоваться стандартным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регулярные выражения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из базовых, но в то же время самых полезных инструментов для предобработки текста - регулярные выражения. В вводной части курса им был посвящен целый семинар - https://github.com/mannefedov/compling_nlp_hse_course/blob/master/notebooks/first_module_intro/01_regular_expressions.ipynb  \n",
    "\n",
    "Если вы не ходили подготовительную часть и чувствуете себя неуверенно при работе с регулярками - пройдитесь по семинару. Решить домашку тоже не помешает. В семинаре я буду предполагать, что вы уже знакомы с вещами, которые разобраны в подготовительной части. Но новые вещи я буду объяснять дополнительно!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение на предложения, токенизация, нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За каждым из этих трех терминов стоит большая и сложная подзадача NLP. Однако для каждой есть готовые решения, которые очень хорошо работают. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE. Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\n",
    "\n",
    "Во-первых, NLI можно использовать для контроля качества генеративных моделей. Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод. Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\". С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\n",
    "\n",
    "Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов. Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше. В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично. Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\n",
    "\n",
    "\"\"\"\n",
    "# текст отсюда - https://habr.com/ru/post/582620/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В __nltk__ есть уже готовая функция для разбивки на предложения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/a.kerimov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/a.kerimov/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download as nltk_download\n",
    "nltk_download('punkt')\n",
    "nltk_download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/a.kerimov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".',\n",
       " 'С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).',\n",
       " 'Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов.',\n",
       " 'Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше.',\n",
       " 'В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично.',\n",
       " 'Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text, 'russian')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk также позволяет обучить свой токенизатор предложений под определенный корпус. Это делается не очень просто, но вот тут есть исчерпывающий туториал - https://nlpforhackers.io/splitting-text-into-sentences/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В __gensim__ тоже есть готовая функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtextcleaner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_sentences\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "from gensim.summarization.textcleaner import split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# это ещё и генератор, т.е. сразу подходит для больших корпусов\n",
    "list(split_sentences(text))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У DeepPavlov есть библиотека [**rusenttokenizer**](https://github.com/deepmipt/ru_sentence_tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rusenttokenize import ru_sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Something went wrong while tokenizing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".',\n",
       " 'С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).',\n",
       " 'Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов.',\n",
       " 'Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше.',\n",
       " 'В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично.',\n",
       " 'Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_sent_tokenize(text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В проекте Natasha есть библиотека [**razdel**](https://github.com/natasha/razdel). Она чуть более навороченная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import sentenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = list(sentenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0,\n",
       "           244,\n",
       "           'Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.'),\n",
       " Substring(245,\n",
       "           322,\n",
       "           'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.'),\n",
       " Substring(324,\n",
       "           401,\n",
       "           'Во-первых, NLI можно использовать для контроля качества генеративных моделей.'),\n",
       " Substring(402,\n",
       "           635,\n",
       "           'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.'),\n",
       " Substring(636,\n",
       "           913,\n",
       "           'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# числа тут - это спаны, индексы начала и конца предложения в изначальном тексте\n",
    "sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# у объекта Substring есть атрибуты start, stop и text. С помощью них можно вытащить нужное\n",
    "[sent.text for sent in sents[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если все-таки нужно добавить каких-то специфичных правил разбиения на предложения, можно опять же воспользоваться регулярными выражениями. Однако в этом случае регулярка будет посложнее. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что получится, если в качестве разделителя использовать !?. пробел и заглавную букву.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE',\n",
       " 'роме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\\n\\nВо-первых, NLI можно использовать для контроля качества генеративных моделей',\n",
       " 'сть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод',\n",
       " 'овременные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\"',\n",
       " ' помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\\n\\nВо-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов',\n",
       " 'ля русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше',\n",
       " ' статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично',\n",
       " 'оэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\\n\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'[!?\\.] [А-Я]', text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема в том, что сам разделитель удаляется тоже, а нам нужно удалить только пробел между знаками препинания и заглавной буквой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решается эта проблема с помощью __look ahead__ и __look behind__ (название функционала в регулярных выражениях).  \n",
    "Синтаксис там такой:  \n",
    " **(?<=pattern)** положительное look-behind условие  \n",
    " **(?<!pattern)** отрицательное look-behind условие   \n",
    " **(?=pattern)** положительное look-ahead условие   \n",
    " **(?!pattern)** отрицательное look-ahead условие   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробно про это написано тут: https://www.regular-expressions.info/lookaround.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look behind и look ahead превращают паттерн в условный, то есть проверяется есть ли он (до или после, соответственно), но его захвата не происходит. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обернём наше регулярное выражение и посмотрим, что получается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".',\n",
       " 'С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).',\n",
       " 'Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов.',\n",
       " 'Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше.',\n",
       " 'В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично.',\n",
       " 'Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.  ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'(?<=[\\.?!]) +(?=[А-ЯЁ])', text.replace('\\n', ' '))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы разбили текст на предложения. Теперь предложения нужно разбить на токены. Под токенами обычно понимаются слова, но это могут быть и какие-то более длинные или короткие куски. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой способ токенизации -- стандартный питоновский __str.split__ метод.  \n",
    "По умолчанию он разбивает текст по последовательностям пробелов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '', '2', '3']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1  2 3'.split(' ') # NB! .split() и .split(' ') - не одно и тоже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1  2 3'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рассмотреть,',\n",
       " 'какие',\n",
       " 'языковые',\n",
       " 'явления',\n",
       " 'данная',\n",
       " 'модель',\n",
       " 'понимает',\n",
       " 'хорошо,',\n",
       " 'а',\n",
       " 'на',\n",
       " 'каких',\n",
       " '–',\n",
       " '\"плывёт\";',\n",
       " 'по',\n",
       " 'этому',\n",
       " 'принципу',\n",
       " 'устроены',\n",
       " 'диагностические',\n",
       " 'датасеты',\n",
       " 'SuperGLUE']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()[10:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большая часть слов отделяется, но знаки препинания лепятся к словам.\n",
    "Можно пройтись по всем словам и убрать из них пунктуацию с методом str.strip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#основные знаки преминания хранятся в питоновском модуле string в punctuation\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в этом списке не хватает кавычек-ёлочек, лапок, длинного тире и многоточия\n",
    "string.punctuation += '«»—…“”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рассмотреть',\n",
       " 'какие',\n",
       " 'языковые',\n",
       " 'явления',\n",
       " 'данная',\n",
       " 'модель',\n",
       " 'понимает',\n",
       " 'хорошо',\n",
       " 'а',\n",
       " 'на',\n",
       " 'каких',\n",
       " '–',\n",
       " 'плывёт',\n",
       " 'по',\n",
       " 'этому',\n",
       " 'принципу',\n",
       " 'устроены',\n",
       " 'диагностические',\n",
       " 'датасеты',\n",
       " 'SuperGLUE']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.strip(string.punctuation) for word in text.split()][10:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так не будут удаляться дефисы и точки в сокращениях, не разделенных пробелом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'как-нибудь'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'как-нибудь'.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'т.е'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'т.е.'.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой способ токенизации может работать быстрее других из-за того, что используются только дефолтные инструменты питона. Если важно качество, то лучше пользоваться готовыми токенизаторами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, готовые токенизаторы есть в nltk. Они не удаляют пунктуацию, а выделяют её отдельным токеном.\n",
    "\n",
    "**wordpunct_tokenizer** разбирает по регулярке - *'\\w+|[^\\w\\s]+'* (попробуйте понять как она работает просто глядя на паттерн)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача',\n",
       " 'NLI',\n",
       " 'важна',\n",
       " 'для',\n",
       " 'компьютерных',\n",
       " 'лингвистов',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволяет']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word_tokenize** также построен на регулярках, но они там более сложные (учитывается последовательность некоторых \n",
    "символов, символы начала, конца слова и т.д). \n",
    "\n",
    "Специально подобранного под русский язык токенизатора там нет, \n",
    "но и с английским всё работает достаточно хорошо --\n",
    "сокращения типа т.к собираются в один токен, дефисные слова тоже не разделяются, многоточия тут тоже не отделяются, но это можно поправить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лажают',\n",
       " ',',\n",
       " 'упуская',\n",
       " 'какую-то',\n",
       " 'важную',\n",
       " 'информацию',\n",
       " 'из',\n",
       " 'Х',\n",
       " ',',\n",
       " 'или',\n",
       " ',',\n",
       " 'наоборот',\n",
       " ',',\n",
       " 'дописывая',\n",
       " 'в',\n",
       " 'текст',\n",
       " 'Y',\n",
       " 'что-то',\n",
       " 'нафантазированное',\n",
       " '``']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)[130:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В генсиме тоже есть функция для токенизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['russiansuperglue',\n",
       " 'кроме',\n",
       " 'этого',\n",
       " 'модели',\n",
       " 'nli',\n",
       " 'обладают',\n",
       " 'прикладной',\n",
       " 'ценностью',\n",
       " 'по',\n",
       " 'нескольким',\n",
       " 'причинам',\n",
       " 'во',\n",
       " 'первых',\n",
       " 'nli',\n",
       " 'можно',\n",
       " 'использовать',\n",
       " 'для',\n",
       " 'контроля',\n",
       " 'качества',\n",
       " 'генеративных']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# опять же, это генератор\n",
    "list(tokenize(text, lowercase=True))[30:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И в razdel тоже есть токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize as razdel_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 6, 'Задача'),\n",
       " Substring(7, 10, 'NLI'),\n",
       " Substring(11, 16, 'важна'),\n",
       " Substring(17, 20, 'для'),\n",
       " Substring(21, 33, 'компьютерных'),\n",
       " Substring(34, 44, 'лингвистов'),\n",
       " Substring(44, 45, ','),\n",
       " Substring(46, 49, 'ибо'),\n",
       " Substring(50, 53, 'она'),\n",
       " Substring(54, 63, 'позволяет')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(razdel_tokenize(text))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача',\n",
       " 'NLI',\n",
       " 'важна',\n",
       " 'для',\n",
       " 'компьютерных',\n",
       " 'лингвистов',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволяет']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in list(razdel_tokenize(text))[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работать с регистром тяжело и поэтому можно привести все к нижнему регистру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача',\n",
       " 'nli',\n",
       " 'важна',\n",
       " 'для',\n",
       " 'компьютерных',\n",
       " 'лингвистов',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволяет']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text.lower() for token in list(razdel_tokenize(text))[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В последнее время нормализация (т.е. приведение токенов к стандартному виду) используется все реже. Это связано с использованием subword или byte токенизации в топовых моделях (подробнее об этом мы поговорим когда дойдем до нейронных сетей). Однако у них есть свои недостатки и забывать про нормализацию пока не стоит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Два основных вида нормализации - лемматизация и стемминг. Стемминг уже нигде не используется, но его полезно разобрать, чтобы понимать почему нужно использовать лемматизацию (еще стемминг по непонятной причине часто упоминается как важный скилл в вакансиях)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стемминг - это урезание слова до его \"основы\" (стема), т.е. такой части, которая является общей для всех словоформ в парадигме слова *(Значения слов \"слово\", \"слоформа\", \"парадигма\" приблизительно соответствует тому, которое использует Зализняк вот тут - http://inslav.ru/images/stories/pdf/2002_Zalizniak_RIS_i_statji.pdf (стр. 21-22)) Но это на самом деле не важно)*. \n",
    "\n",
    "По крайней мере так в теории. На практике стемминг сводится к отбрасыванию частотных окончаний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый известный стеммер - стеммер Портера (или snowball стеммер). \n",
    "Подробнее про стеммер Портера можно почитать вот тут - <https://medium.com/@eigenein/стеммер-портера-для-русского-языка-d41c38b2d340>  \n",
    "А совсем подробнее вот тут - <http://snowball.tartarus.org/algorithms/russian/stemmer.html>  \n",
    "Почему он так называется? Так назывался язык программирования, который Портер написал для стеммеров. Язык так называется в созвучие языку SNOBOL. Вот комментарий самого Портера:\n",
    "\n",
    "`Since it effectively provides a ‘suffix STRIPPER GRAMmar’, I had toyed with the idea of calling it ‘strippergram’, but good sense has prevailed, and so it is ‘Snowball’ named as a tribute to SNOBOL, the excellent string handling language of Messrs Farber, Griswold, Poage and Polonsky from the 1960s.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовые стеммеры для разных языков есть в nltk. Работают они вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Задача', 'задач'),\n",
       " ('NLI', 'NLI'),\n",
       " ('важна', 'важн'),\n",
       " ('для', 'для'),\n",
       " ('компьютерных', 'компьютерн'),\n",
       " ('лингвистов', 'лингвист'),\n",
       " (',', ','),\n",
       " ('ибо', 'иб'),\n",
       " ('она', 'он'),\n",
       " ('позволяет', 'позволя'),\n",
       " ('детально', 'детальн'),\n",
       " ('рассмотреть', 'рассмотрет'),\n",
       " (',', ','),\n",
       " ('какие', 'как'),\n",
       " ('языковые', 'языков'),\n",
       " ('явления', 'явлен'),\n",
       " ('данная', 'дан'),\n",
       " ('модель', 'модел'),\n",
       " ('понимает', 'понима'),\n",
       " ('хорошо', 'хорош'),\n",
       " (',', ','),\n",
       " ('а', 'а'),\n",
       " ('на', 'на'),\n",
       " ('каких', 'как'),\n",
       " ('–', '–'),\n",
       " ('``', '``'),\n",
       " ('плывёт', 'плывет'),\n",
       " (\"''\", \"''\"),\n",
       " (';', ';'),\n",
       " ('по', 'по')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(word, stemmer.stem(word)) for word in word_tokenize(text)][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостатки стемминга достаточно очевидные:  \n",
    "1) с супплетивными формами или редкими окончаниями слова стемминг работать не умеет  \n",
    "2) к одной основе могут приводится разные слова  \n",
    "3) к разным основам могут сводиться формы одного слова  \n",
    "4) приставки не отбрасываются"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация - это замена словоформы слова в парадигме на какую-то заранее выбранную стадартную форму (лемму). \n",
    "\n",
    "\n",
    "\n",
    "Например, для разных форм глагола леммой обычно является неопределенная форма (инфинитив), а для существительного форма мужского рода единственного числа. Это позволяет избавиться от недостатков стемминга (будет, был - одна лемма), (пролить, пролом - разные). Однако лемматизация значительно сложнее. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К счастью есть готовые хорошие лемматизаторы. Для русского основых варианта два: Mystem и Pymorphy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import os, json\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Майстем работает немного лучше и сам токенизирует,\n",
    "поэтому можно в него засовывать сырой текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача', ' ', 'NLI', ' ', 'важный', ' ', 'для', ' ', 'компьютерный', ' ']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mystem.lemmatize функция лемматизации в майстеме\n",
    "# сам объект mystem нужно заранее инициализировать\n",
    "mystem.lemmatize(text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужна грамматическая информация или надо сохранить ненормализованный текст,\n",
    "# есть функция mystem.analyze\n",
    "words_analized = mystem.analyze(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'задача', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}],\n",
       "  'text': 'Задача'},\n",
       " {'text': ' '},\n",
       " {'analysis': [], 'text': 'NLI'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'важный', 'wt': 1, 'gr': 'A=ед,кр,жен'}],\n",
       "  'text': 'важна'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'для', 'wt': 1, 'gr': 'PR='}], 'text': 'для'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'компьютерный',\n",
       "    'wt': 1,\n",
       "    'gr': 'A=(пр,мн,полн|вин,мн,полн,од|род,мн,полн)'}],\n",
       "  'text': 'компьютерных'},\n",
       " {'text': ' '}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# возвращает она список словарей\n",
    "# каждый словарь имеет либо одно поле 'text' (когда попался пробел) или text и analysis\n",
    "# в analysis снова список словарей с вариантами разбора (первый самый вероятный)\n",
    "# поля в analysis - 'gr' - грамматическая информация, 'lex' - лемма\n",
    "# analysis - может быть пустым списком\n",
    "words_analized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово -  Задача\n",
      "Разбор слова -  {'lex': 'задача', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}\n",
      "Лемма слова -  задача\n",
      "Грамматическая информация слова -  S,жен,неод=им,ед\n"
     ]
    }
   ],
   "source": [
    "print('Слово - ', words_analized[0]['text'])\n",
    "print('Разбор слова - ', words_analized[0]['analysis'][0])\n",
    "print('Лемма слова - ', words_analized[0]['analysis'][0]['lex'])\n",
    "print('Грамматическая информация слова - ', words_analized[0]['analysis'][0]['gr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача',\n",
       " 'важный',\n",
       " 'для',\n",
       " 'компьютерный',\n",
       " 'лингвист',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволять',\n",
       " 'детально',\n",
       " 'рассматривать']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#леммы можно достать в одну строчку\n",
    "[parse['analysis'][0]['lex'] for parse in words_analized if parse.get('analysis')][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mystem умеет разбивать текст на предложения, но через питоновский интерфейс это сделать не получится. Нужно скачать mystem отсюда - https://yandex.ru/dev/mystem/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого сохранить текст в файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('text.txt', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из командной строки или из питона запустить майстем на нашем файле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# про параметры почитайте в !mystem -h\n",
    "!/Users/a.kerimov/.local/bin/mystem -isc --format json text.txt text_parsed.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целевом файле теперь лежит разобранный текст в jsonlines (json на каждой строчке)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "t = [json.loads(line) for line in open('text_parsed.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'задача', 'gr': 'S,жен,неод=им,ед'}], 'text': 'Задача'},\n",
       " {'text': ' '},\n",
       " {'analysis': [], 'text': 'NLI'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'важный', 'gr': 'A=ед,кр,жен'}], 'text': 'важна'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'для', 'gr': 'PR='}], 'text': 'для'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'компьютерный', 'gr': 'A=пр,мн,полн'},\n",
       "   {'lex': 'компьютерный', 'gr': 'A=вин,мн,полн,од'},\n",
       "   {'lex': 'компьютерный', 'gr': 'A=род,мн,полн'}],\n",
       "  'text': 'компьютерных'},\n",
       " {'text': ' '}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый объект в этом списке - параграф. Каждый параграф на предложения можно разбив по тегу '//s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё так вызывать майстем может понадобиться, если важна скорость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостатки Mystem: это продукт Яндекса с некоторыми ограничениями на использование, больше он не развивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важным достоинством Mystem является то, что он работает не с отдельными словами, а с целым предложением. При определении нужной леммы учитывается контекст, что позволяет во многих случаях разрешать омонимию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pymorphy - открытый и развивается (но не очень активно, т.к. это все сложно)\n",
    "\n",
    "Ссылка на репозиторий: https://github.com/kmike/pymorphy2\n",
    "\n",
    "Попробуйте сразу установить быструю версию (pip install pymorphy2[fast])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У него нет втстроенной токенизации и он расценивает всё как слово. Когда есть несколько вариантов, он выдает их с вероятностостями, которые расчитатны на корпусе со снятой неоднозначностью. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# основная функция - pymorphy.parse\n",
    "words_analized = [morph.parse(token) for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='печь', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='печь', score=0.571428, methods_stack=((DictionaryAnalyzer(), 'печь', 2223, 0),)),\n",
       " Parse(word='печь', tag=OpencorporaTag('INFN,impf,tran'), normal_form='печь', score=0.285714, methods_stack=((DictionaryAnalyzer(), 'печь', 2456, 0),)),\n",
       " Parse(word='печь', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form='печь', score=0.142857, methods_stack=((DictionaryAnalyzer(), 'печь', 2223, 3),))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse(\"печь\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первое слово -  задача\n",
      "Разбор первого слова -  Parse(word='задача', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='задача', score=1.0, methods_stack=((DictionaryAnalyzer(), 'задача', 94, 0),))\n",
      "Лемма первого слова -  задача\n",
      "Грамматическая информация первого слова -  NOUN,inan,femn sing,nomn\n",
      "Часть речи первого слова -  NOUN\n",
      "Род первого слова -  femn\n",
      "Число первого слова -  sing\n",
      "Падеж первого слова -  nomn\n"
     ]
    }
   ],
   "source": [
    "# Она похожа на analyze в майстеме только возрващает список объектов Parse\n",
    "# Первый в списке - самый вероятный разбор (у каждого есть score)\n",
    "# Информация достается через атрибут (Parse.word - например)\n",
    "# Грамматическая информация хранится в объекте OpencorporaTag и из него удобно доставать\n",
    "# части речи или другие категории\n",
    "print('Первое слово - ', words_analized[0][0].word)\n",
    "print('Разбор первого слова - ', words_analized[0][0])\n",
    "print('Лемма первого слова - ', words_analized[0][0].normal_form)\n",
    "print('Грамматическая информация первого слова - ', words_analized[0][0].tag)\n",
    "print('Часть речи первого слова - ', words_analized[0][0].tag.POS)\n",
    "print('Род первого слова - ', words_analized[0][0].tag.gender)\n",
    "print('Число первого слова - ', words_analized[0][0].tag.number)\n",
    "print('Падеж первого слова - ', words_analized[0][0].tag.case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная очистка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно убрать стоп-слова (предлоги, союзы, местоимения, частотные слова). Сам термин стоп-слово происходит из информационного поиска, первый раз его упомянул [Питер Лун](https://en.wikipedia.org/wiki/Hans_Peter_Luhn) в 1959.  \n",
    "Удаление таких слов позволяло сократить размер индекса и не сильно испортить выдачу или даже улучшить её, поднимая релевантность документам со значимыми словами. Со временем от такой практики, в основном, отказались - память стала дешевой (и повились всякие алгоритмы для сокращения потребления памяти), а для учёта значимости придумали TFIDF (про него на следующем занятии).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во многих поисковых движках стоп-слова всё ещё используются. Часто их используют и в практических задачах (классификации, тематическом моделировании). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "# стоп-слова есть в nltk\n",
    "stops = stopwords.words('russian')\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список не идеальный и его можно расширять под свои задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача',\n",
       " 'nli',\n",
       " 'важный',\n",
       " 'компьютерный',\n",
       " 'лингвист',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'позволять',\n",
       " 'детально',\n",
       " 'рассмотреть']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_normalized = [morph.parse(token)[0].normal_form for token in word_tokenize(text)]\n",
    "[word for word in words_normalized if word not in stops][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка для других языков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка сильно зависит от языка. Полностью универсальнных токенизаторов, лемматизаторов не бывает, а рассказать о предобработке под все существующие языки не реально. Поэтому посмотрим дополнительно только на предобработку на английском языке. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk и gensim по умолчанию адаптированы под английский язык (а регулярки вообще не привязаны к языку), поэтому разбирать их еще раз не будем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека, про которую стоит отдельно рассказать - [**SpaCy**](https://spacy.io/). Это многоцелевая многоязычная библиотека. Если вам понадобится серьезно работать с английским, то лучшим вариантом будет использовать SpaCy. Другие языки там тоже поддерживаются (см. документацию), но не настолько хорошо как английский язык. \n",
    "\n",
    "В SpaCy много всего и мы будем возвращаться к ней по ходу курса. Пока посмотрим на интрументы базовой предобработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp310-cp310-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.6.3-py3-none-any.whl.metadata (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m966.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.16.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Downloading spacy-3.7.4-cp310-cp310-macosx_11_0_arm64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp310-cp310-macosx_11_0_arm64.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.10-cp310-cp310-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-macosx_11_0_arm64.whl (127 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.2/395.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.3-cp310-cp310-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp310-cp310-macosx_11_0_arm64.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.3-cp310-cp310-macosx_11_0_arm64.whl (789 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.6/789.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp310-cp310-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, langcodes, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 7.0.1\n",
      "    Uninstalling smart-open-7.0.1:\n",
      "      Successfully uninstalled smart-open-7.0.1\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.6.3 pydantic-core-2.16.3 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from de-core-news-sm==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Collecting ru-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.7.0/ru_core_news_sm-3.7.0-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from ru-core-news-sm==3.7.0) (3.7.4)\n",
      "Collecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.7.0)\n",
      "  Downloading pymorphy3-2.0.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0) (0.7.2)\n",
      "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n",
      "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/a.kerimov/bmstu/IRS/env-3.10/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.1.5)\n",
      "Downloading pymorphy3-2.0.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m811.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, pymorphy3, ru-core-news-sm\n",
      "Successfully installed pymorphy3-2.0.1 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем пайплайн для английского языка\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = (\"One of the most salient features of our culture is that it won't so much bullshit.” \"\n",
    "        \"These are the opening words of the short book On Bullshit, written by the philosopher Harry Frankfurt. \"\n",
    "        \"Fifteen years after the publication of this surprise bestseller, \"\n",
    "        \"the rapid progress of research on artificial intelligence is forcing us to reconsider our conception \"\n",
    "        \"of bullshit as a hallmark of human speech, with troubling implications. What do philosophical \"\n",
    "        \"reflections on bullshit have to do with algorithms? As it turns out, quite a lot.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат предобаботки очень похож на майстем - тут есть разбиение на предложения, на токены, лемматизация, определение части речи. Тэги тут используются другие, но при желании можно сделать более менее адекватный маппинг (возможно кто-то уже это сделал и нужно только погуглить)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One one NUM\n",
      "of of ADP\n",
      "the the DET\n",
      "most most ADV\n",
      "salient salient ADJ\n",
      "features feature NOUN\n",
      "of of ADP\n",
      "our our PRON\n",
      "culture culture NOUN\n",
      "is be AUX\n",
      "that that SCONJ\n",
      "it it PRON\n",
      "wo will AUX\n",
      "n't not PART\n",
      "so so ADV\n",
      "much much ADJ\n",
      "bullshit bullshit NOUN\n",
      ". . PUNCT\n",
      "” \" PUNCT\n",
      "\n",
      "These these PRON\n",
      "are be AUX\n",
      "the the DET\n",
      "opening opening NOUN\n",
      "words word NOUN\n",
      "of of ADP\n",
      "the the DET\n",
      "short short ADJ\n",
      "book book NOUN\n",
      "On on ADP\n",
      "Bullshit Bullshit PROPN\n",
      ", , PUNCT\n",
      "written write VERB\n",
      "by by ADP\n",
      "the the DET\n",
      "philosopher philosopher NOUN\n",
      "Harry Harry PROPN\n",
      "Frankfurt Frankfurt PROPN\n",
      ". . PUNCT\n",
      "\n",
      "Fifteen fifteen NUM\n",
      "years year NOUN\n",
      "after after ADP\n",
      "the the DET\n",
      "publication publication NOUN\n",
      "of of ADP\n",
      "this this DET\n",
      "surprise surprise NOUN\n",
      "bestseller bestseller NOUN\n",
      ", , PUNCT\n",
      "the the DET\n",
      "rapid rapid ADJ\n",
      "progress progress NOUN\n",
      "of of ADP\n",
      "research research NOUN\n",
      "on on ADP\n",
      "artificial artificial ADJ\n",
      "intelligence intelligence NOUN\n",
      "is be AUX\n",
      "forcing force VERB\n",
      "us we PRON\n",
      "to to PART\n",
      "reconsider reconsider VERB\n",
      "our our PRON\n",
      "conception conception NOUN\n",
      "of of ADP\n",
      "bullshit bullshit NOUN\n",
      "as as ADP\n",
      "a a DET\n",
      "hallmark hallmark NOUN\n",
      "of of ADP\n",
      "human human ADJ\n",
      "speech speech NOUN\n",
      ", , PUNCT\n",
      "with with ADP\n",
      "troubling troubling ADJ\n",
      "implications implication NOUN\n",
      ". . PUNCT\n",
      "\n",
      "What what PRON\n",
      "do do AUX\n",
      "philosophical philosophical ADJ\n",
      "reflections reflection NOUN\n",
      "on on ADP\n",
      "bullshit bullshit NOUN\n",
      "have have VERB\n",
      "to to PART\n",
      "do do VERB\n",
      "with with ADP\n",
      "algorithms algorithm NOUN\n",
      "? ? PUNCT\n",
      "\n",
      "As as SCONJ\n",
      "it it PRON\n",
      "turns turn VERB\n",
      "out out ADP\n",
      ", , PUNCT\n",
      "quite quite DET\n",
      "a a DET\n",
      "lot lot NOUN\n",
      ". . PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задачах вроде извлечения ключевых слов может пригодится вытащить из текста только noun phrases (по-русски это вроде называется именные группы существительного)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['the most salient features', 'our culture', 'it', 'These', 'the opening words', 'the short book', 'Bullshit', 'the philosopher', 'Harry Frankfurt', 'the publication', 'this surprise bestseller', 'the rapid progress', 'research', 'artificial intelligence', 'us', 'our conception', 'bullshit', 'a hallmark', 'human speech', 'troubling implications', 'What', 'philosophical reflections', 'bullshit', 'algorithms', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем пайплайн для немецкого языка\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = (\"Vor den Stadien habe ich bis jetzt zum Glück noch keine wüsten Szenen gesehen.\"\n",
    "        \"Vorstandschef Timotheus Höttges habe sich ausgesprochen optimistisch gezeigt, schrieb Analyst Robert Grindle in einer Studie vom Montag. \"\n",
    "        \"Während der dortigen Räterepublik war er nach dem Krieg in Künstlergruppen und Ausschüssen aktiv.\"\n",
    "        \"Welches Ergebnis die Diskussion auf EU-Ebene auch letztlich bringt, wichtig ist, dass die Preisentwicklung für die\"\n",
    "        \"Menschen verträglicher gestaltet wird“, so Gusenbauer.\"\n",
    "        \"Weitere Informationen unter www.schnippenburg.de sowie www.eisenzeithaus.de. Es gibt neue Nachrichten auf noz.de!\" \n",
    "        \"Jetzt die Startseite neu laden.\"\n",
    "        \"Der Initiative 'Zivilcourage', die sich jahrelang für das Denkmal in Form eines offenen \" \n",
    "        \"Der islamistischen Szene Thüringens wurden nach Angaben des Thüringer Innenministeriums \"\n",
    "        \"zuletzt etwa 125 Personen zugerechnet, der salafistischen Szene etwa 75 Personen.\"\n",
    "        \"Allerdings bestand er die EMV-Prüfung nicht, weil er Radios und DVB-T-Empfänger stört.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vor vor ADP\n",
      "den der DET\n",
      "Stadien Stadien NOUN\n",
      "habe haben AUX\n",
      "ich ich PRON\n",
      "bis bis ADP\n",
      "jetzt jetzt ADV\n",
      "zum zu ADP\n",
      "Glück Glück NOUN\n",
      "noch noch ADV\n",
      "keine kein DET\n",
      "wüsten wüst ADJ\n",
      "Szenen Szene NOUN\n",
      "gesehen sehen VERB\n",
      ". -- PUNCT\n",
      "\n",
      "Vorstandschef Vorstandschef NOUN\n",
      "Timotheus Timotheus PROPN\n",
      "Höttges Höttges PROPN\n",
      "habe haben AUX\n",
      "sich sich PRON\n",
      "ausgesprochen ausgesprochen ADV\n",
      "optimistisch optimistisch ADV\n",
      "gezeigt zeigen VERB\n",
      ", -- PUNCT\n",
      "schrieb schreiben VERB\n",
      "Analyst Analyst NOUN\n",
      "Robert Robert PROPN\n",
      "Grindle Grindle PROPN\n",
      "in in ADP\n",
      "einer ein DET\n",
      "Studie Studie NOUN\n",
      "vom von ADP\n",
      "Montag Montag NOUN\n",
      ". -- PUNCT\n",
      "\n",
      "Während während ADP\n",
      "der der DET\n",
      "dortigen dortig ADJ\n",
      "Räterepublik Räterepublik NOUN\n",
      "war sein AUX\n",
      "er er PRON\n",
      "nach nach ADP\n",
      "dem der DET\n",
      "Krieg Krieg NOUN\n",
      "in in ADP\n",
      "Künstlergruppen Künstlergruppe NOUN\n",
      "und und CCONJ\n",
      "Ausschüssen Ausschuß NOUN\n",
      "aktiv aktiv ADV\n",
      ". -- PUNCT\n",
      "\n",
      "Welches welcher DET\n",
      "Ergebnis Ergebnis NOUN\n",
      "die der DET\n",
      "Diskussion Diskussion NOUN\n",
      "auf auf ADP\n",
      "EU-Ebene EU-Eben NOUN\n",
      "auch auch ADV\n",
      "letztlich letztlich ADV\n",
      "bringt bringen VERB\n",
      ", -- PUNCT\n",
      "wichtig wichtig ADV\n",
      "ist sein AUX\n",
      ", -- PUNCT\n",
      "dass dass SCONJ\n",
      "die der DET\n",
      "Preisentwicklung Preisentwicklung NOUN\n",
      "für für ADP\n",
      "dieMenschen dieMenschen NOUN\n",
      "verträglicher verträglich ADV\n",
      "gestaltet gestalten VERB\n",
      "wird werden AUX\n",
      "“ -- PUNCT\n",
      ", -- PUNCT\n",
      "so so ADV\n",
      "Gusenbauer Gusenbauer PROPN\n",
      ". -- PUNCT\n",
      "\n",
      "Weitere weit ADJ\n",
      "Informationen Information NOUN\n",
      "unter unter ADP\n",
      "www.schnippenburg.de www.schnippenburg.de PROPN\n",
      "sowie sowie CCONJ\n",
      "www.eisenzeithaus.de www.eisenzeithaus.de NOUN\n",
      ". -- PUNCT\n",
      "\n",
      "Es es PRON\n",
      "gibt geben VERB\n",
      "neue neu ADJ\n",
      "Nachrichten Nachricht NOUN\n",
      "auf auf ADP\n",
      "noz.de noz.de PROPN\n",
      "! -- PUNCT\n",
      "\n",
      "Jetzt jetzt ADV\n",
      "die der DET\n",
      "Startseite Startseit NOUN\n",
      "neu neu ADV\n",
      "laden laden VERB\n",
      ". -- PUNCT\n",
      "\n",
      "Der der DET\n",
      "Initiative Initiative NOUN\n",
      "' ' PUNCT\n",
      "Zivilcourage Zivilcourage NOUN\n",
      "' -- PUNCT\n",
      ", -- PUNCT\n",
      "die der PRON\n",
      "sich sich PRON\n",
      "jahrelang jahrelang ADV\n",
      "für für ADP\n",
      "das der DET\n",
      "Denkmal Denkmal NOUN\n",
      "in in ADP\n",
      "Form Form NOUN\n",
      "eines ein DET\n",
      "offenen offen ADJ\n",
      "Der der DET\n",
      "islamistischen islamistisch ADJ\n",
      "Szene Szene NOUN\n",
      "Thüringens Thüringen PROPN\n",
      "wurden werden AUX\n",
      "nach nach ADP\n",
      "Angaben Angabe NOUN\n",
      "des der DET\n",
      "Thüringer Thüringer ADJ\n",
      "Innenministeriums Innenministerium NOUN\n",
      "zuletzt zuletzt ADV\n",
      "etwa etwa ADV\n",
      "125 125 NUM\n",
      "Personen Person NOUN\n",
      "zugerechnet zurechnen VERB\n",
      ", -- PUNCT\n",
      "der der DET\n",
      "salafistischen salafistisch ADJ\n",
      "Szene Szene NOUN\n",
      "etwa etwa ADV\n",
      "75 75 NUM\n",
      "Personen Person NOUN\n",
      ". -- PUNCT\n",
      "\n",
      "Allerdings allerdings ADV\n",
      "bestand bestehen VERB\n",
      "er er PRON\n",
      "die der DET\n",
      "EMV-Prüfung EMV-Prüfung NOUN\n",
      "nicht nicht PART\n",
      ", -- PUNCT\n",
      "weil weil SCONJ\n",
      "er er PRON\n",
      "Radios Radios NOUN\n",
      "und und CCONJ\n",
      "DVB-T-Empfänger DVB-T-Empfäng NUM\n",
      "stört stören VERB\n",
      ". -- PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['den Stadien', 'ich', 'Glück', 'noch keine wüsten Szenen', 'Vorstandschef Timotheus Höttges', 'sich', 'Analyst Robert Grindle', 'einer Studie', 'Montag', 'der dortigen Räterepublik', 'er', 'dem Krieg', 'Künstlergruppen', 'Ausschüssen', 'Welches Ergebnis', 'die Diskussion', 'EU-Ebene', 'die Preisentwicklung', 'dieMenschen', 'Weitere Informationen', 'www.schnippenburg.de', 'www.eisenzeithaus.de', 'neue Nachrichten', 'noz.de', 'die Startseite', \"Der Initiative 'Zivilcourage\", 'die', 'sich', 'das Denkmal', 'Form', 'Der islamistischen Szene', 'Thüringens', 'Angaben', 'des Thüringer Innenministeriums', 'etwa 125 Personen', 'der salafistischen Szene', 'etwa 75 Personen', 'er', 'die EMV-Prüfung', 'er', 'Radios']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поддержка русского языка в spacy тоже не так давно добавилась, но она не полная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем любой текст\n",
    "text = \"ДАННОЕ СООБЩЕНИЕ (МАТЕРИАЛ) СОЗДАНО И (ИЛИ) РАСПРОСТРАНЕНО \"\\\n",
    "       \"ИНОСТРАННЫМ СРЕДСТВОМ МАССОВОЙ ИНФОРМАЦИИ, ВЫПОЛНЯЮЩИМ \"\\\n",
    "       \"ФУНКЦИИ ИНОСТРАННОГО АГЕНТА, И (ИЛИ) РОССИЙСКИМ ЮРИДИЧЕСКИМ ЛИЦОМ, \"\\\n",
    "       \"ВЫПОЛНЯЮЩИМ ФУНКЦИИ ИНОСТРАННОГО АГЕНТА\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ДАННОЕ данное NOUN\n",
      "СООБЩЕНИЕ сообщение PROPN\n",
      "( ( PUNCT\n",
      "МАТЕРИАЛ материал PROPN\n",
      ") ) PUNCT\n",
      "СОЗДАНО создано PROPN\n",
      "И и PROPN\n",
      "( ( PUNCT\n",
      "ИЛИ или PROPN\n",
      ") ) PUNCT\n",
      "РАСПРОСТРАНЕНО распространено PROPN\n",
      "ИНОСТРАННЫМ иностранным PROPN\n",
      "СРЕДСТВОМ средством PROPN\n",
      "МАССОВОЙ массовой PROPN\n",
      "ИНФОРМАЦИИ информации PROPN\n",
      ", , PUNCT\n",
      "ВЫПОЛНЯЮЩИМ выполняющим PROPN\n",
      "ФУНКЦИИ функция PROPN\n",
      "ИНОСТРАННОГО иностранного PROPN\n",
      "АГЕНТА агента PROPN\n",
      ", , PUNCT\n",
      "И и CCONJ\n",
      "( ( PUNCT\n",
      "ИЛИ или PROPN\n",
      ") ) PUNCT\n",
      "РОССИЙСКИМ российским PROPN\n",
      "ЮРИДИЧЕСКИМ юридическим PROPN\n",
      "ЛИЦОМ лицом PROPN\n",
      ", , PUNCT\n",
      "ВЫПОЛНЯЮЩИМ выполняющим PROPN\n",
      "ФУНКЦИИ функция PROPN\n",
      "ИНОСТРАННОГО иностранного PROPN\n",
      "АГЕНТА агент PROPN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "[E894] The 'noun_chunks' syntax iterator is not implemented for language 'ru'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoun phrases:\u001b[39m\u001b[38;5;124m\"\u001b[39m, [chunk\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mnoun_chunks])\n",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoun phrases:\u001b[39m\u001b[38;5;124m\"\u001b[39m, [chunk\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mnoun_chunks])\n",
      "File \u001b[0;32m~/bmstu/IRS/env-3.10/lib/python3.10/site-packages/spacy/tokens/doc.pyx:900\u001b[0m, in \u001b[0;36mnoun_chunks\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: [E894] The 'noun_chunks' syntax iterator is not implemented for language 'ru'."
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
